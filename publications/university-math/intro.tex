\part{Introduction}
\chapter{Mathematical Reasoning and Logic}
% https://www.maths.ox.ac.uk/system/files/attachments/study_public_0.pdf

\section{Logical statements and notation}
Some terminology:
\begin{itemize}
\item \textbf{Definition}: a precise and unambiguous description of the meaning of a mathematical term. It characterises the meaning of a word by giving all the properties and only those properties that must be true.

\item \textbf{Theorem}: a true mathematical statement that can be proven mathematically. In a mathematical paper, the term theorem is often reserved for the most important results.

\item \textbf{Lemma}: a minor result whose sole purpose is to help in proving a theorem. It is a stepping stone on the path to proving a theorem. Very occasionally lemmas can take on a life of their own.

\item \textbf{Corollary}: a result in which the (usually short) proof relies heavily on a given theorem. We often say that ``this is a corollary of Theorem A".

\item \textbf{Proposition}: a proven and often interesting result, but generally less important than a theorem.

\item \textbf{Conjecture}: a statement that is unproved, but is believed to be true.

\item \textbf{Axiom/Postulate}: a statement that is assumed to be true without proof. These are the basic building blocks from which all theorems are proven.

\item \textbf{Identity}: a mathematical expression giving the equality of two (often variable) quantities.

\item \textbf{Paradox}: a statement that can be shown, using a given set of axioms and definitions, to be both true and false. Paradoxes are often used to show the inconsistencies in a flawed theory.
\end{itemize}

\subsection{Notation}
A proposition is a sentence which has exactly one truth value, i.e. it is either true or false, but not both and not neither. A proposition is denoted by uppercase letters such as $P$ and $Q$. If the proposition $P$ depends on a variable $x$, it is sometimes helpful to denote it by $P(x)$. 

\textbf{Equivalence:} $P \equiv Q$ means $P$ and $Q$ are logically equivalent statements.

\textbf{Conjunction:} $P \land Q$ means ``$P$ and $Q$".

\textbf{Disjunction:} $P \lor Q$ means ``$P$ or $Q$".

\textbf{Negation:} $\lnot P$ means ``not $P$".

Here are some useful properties when handling logical statements.
\begin{itemize}
\item Double negation law:
\[ P \equiv \lnot(\lnot P) \]

\item Commutative property:
\[ P \land Q \equiv Q \land P, \quad P \lor Q \equiv Q \lor P \]

\item Associative property for conjunction: 
\[ (P\land Q)\land R \equiv P\land (Q\land R) \]

\item Associative property for disjunction: 
\[ (P\lor Q)\lor R \equiv P\lor (Q\lor R) \]

\item Distributive property for conjunction across disjunction: 
\[ P\land(Q\lor R) \equiv (P\land Q)\lor(P\land Q) \]

\item Distributive property for disjunction across conjunction: 
\[ P\lor(Q\land R) \equiv (P\lor Q)\land(P\lor R) \]

\item \textbf{De Morgan's Laws}:
\[ \lnot(P \lor Q) \iff (\lnot P \land \lnot Q) \]
\[ \lnot (P\land Q) \iff (\lnot P\lor \lnot Q) \]
\end{itemize}

\begin{exmp}{}{}
Assume that $x$ is a fixed real number. What is the negation of the statement $1<x<2$?
\end{exmp}
\begin{solution}
The negation of $1<x<2$ is ``it is not the case that $1<x<2$”. This is not useful.

Note that $1<x<2$ means $1<x$ and $x<2$. Let $P:1<x$ and $Q:x<2$. Then the statement $1<x<2$ is $P \land Q$.

By De Morgan's Laws, we have $\lnot (P \land Q) \equiv \lnot P \lor \lnot Q$.

\textbf{Trichotomy Axiom of real numbers} states that given fixed real numbers $a$ and $b$, exactly one of the statements $a<b, a=b, b<a$ is true. Hence $\lnot P \equiv \lnot (1<x) \equiv (x \le 1)$ and $\lnot Q \equiv \lnot (x<2) \equiv (x \ge 2)$.

Thus
\[ \lnot (1<x<2) \equiv \lnot (P \land Q) \equiv \lnot P \lor \lnot Q \equiv (1 \ge x) \lor (x \ge 2). \]

Therefore the negation of $1<x<2$ is logically equivalent to the statement $x \le 1$ or $x \ge 2$.
\end{solution}

\begin{exmp}{}{}
Assume that $n$ is a fixed positive integer. Find a useful denial of the statement
\[ n = 2 \text{ or } n \text{ is odd.} \]
\end{exmp}
\begin{solution}
Using De Morgan's Laws,
\begin{align*}
\lnot [(n = 2) \lor (n \text{ is odd})] &\equiv \lnot(n = 2) \land \lnot(n \text{ is odd}) \\
&\equiv (n \neq 2) \land (n \text{ is even})
\end{align*}
where we are using the fact that every integer is either even or odd, but not both.

Thus a useful denial of the given statement is: $n$ is an even integer other than 2.
\end{solution}
\pagebreak

\subsection{If, only if, $\implies$}
\textbf{Implication:} $P \implies Q$ means ``$P$ implies $Q$", i.e. if $P$ holds then $Q$ also holds. It is equivalent to saying ``If $P$ then $Q$". The only case when $P \implies Q$ is false is when the hypothesis $P$ is true and the conclusion $Q$ is false.

$P \implies Q$ is known as a \textbf{conditional statement}. $P$ is known as the \textbf{hypothesis}, $Q$ is known as the \textbf{conclusion}.

Statements of this form are probably the most common, although they may sometimes appear quite differently. The following all mean the same thing:
\begin{enumerate}[label=(\roman*)]
\item if $P$ then $Q$;
\item $P$ implies $Q$;
\item $P$ only if $Q$;
\item $P$ is a sufficient condition for $Q$;
\item $Q$ is a necessary condition for $P$.
\end{enumerate}

The \textbf{converse} of $P \implies Q$ is given by $Q \implies P$; both are \emph{not} logically equivalent.

The \textbf{inverse} of $P \implies Q$ is given by $\lnot P \implies \lnot Q$, i.e. the hypothesis and conclusion of the statement are both negated.

The \textbf{contrapositive} of $P \implies Q$ is given by $\lnot Q \implies \lnot P$; both are logically equivalent.

\textbf{How to prove:} To prove $P \implies Q$, start by assuming that $P$ holds and try to deduce through some logical steps that $Q$ holds too. Alternatively, start by assuming that $Q$ does not hold and show that $P$ does not hold (that is, we prove the contrapositive).
\pagebreak

\subsection{If and only if, iff, $\iff$}
\textbf{Bidirectional implication:} $P \iff Q$ means $P \implies Q$ and $Q \implies P$. We can read this as ``$P$ if and only if $Q$". The letters ``iff" are also commonly used to stand for ‘if and only if’.
\[ P \iff Q \equiv (P \implies Q) \land (Q \implies P) \]

$P \iff Q$ is true exactly when $P$ and $Q$ have the same truth value.

$P \iff Q$ is known as a \textbf{biconditional statement}.

These statements are usually best thought of separately as ‘if’ and ‘only if’ statements.

\textbf{How to prove:} To prove $P \iff Q$, prove the statement in both directions, i.e. prove both $P \implies Q$ and $Q \implies P$. Remember to make very clear, both to yourself and in your written proof, which direction you are doing.
\pagebreak

\subsection{Quantifiers}
\textbf{Quantifiers}: universal quantifier $\forall$ means ``for all" or ``for every", universal quantifier $\exists$ means ``there exists". For example, ``$\exists x \in S \suchthat P(x)$" can be read as ``there exists $x$ in $S$ such that $P(x)$ holds". A common variant is $\exists!$ which means ``there exists unique", implying that there is one, and only one, element with the given property.

These are versions of \textbf{De Morgan's laws} for quantifiers:
\[ \lnot \forall x\,P(x) \iff \exists x\,\lnot P(x) \]
\[ \lnot \exists x\,P(x) \iff \forall x\,\lnot P(x) \]

\begin{exmp}{}{}
Find a useful denial of the statement
\[ \text{for all real numbers } x, \text{ if } x>2, \text{ then } x^2>4 \]
\end{exmp}
\begin{solution}
In logical notation, this statement is $(\forall x \in \RR)[x>2 \implies x^2>4]$.
\begin{align*}
\lnot\{(\forall x \in \RR)[x>2 \implies x^2>4]\} 
&\equiv (\exists x \in \RR) \lnot[x>2 \implies x^2>4] \\
&\equiv (\exists x \in \RR) \lnot [(x\le2) \lor (x^2>4)] \\
&\equiv (\exists x \in \RR) [(x>2) \land (x^2\le4)]
\end{align*}
Therefore a useful denial of the statement is:
\[ \text{there exists a real number } x \text{ such that } x>2 \text{ and } x^2\le4. \] 
\end{solution}

\begin{exmp}{}{}
Negate surjectivity.
\end{exmp}
\begin{solution}
If $f:X\to Y$ is not surjective, then it means that there exists $y \in Y$ not in the image of $X$, i.e. for all $x$ in $X$ we have $f(x)\neq y$.
\begin{align*}
\lnot \forall y \in Y, \exists x \in X, f(x)=y 
&\iff \exists y \in Y, \lnot (\exists x \in X, f(x)=y) \\
&\iff \exists y \in Y, \forall x \in X, \lnot (f(x)=y) \\
&\iff \exists y \in Y, \forall x \in X, f(x) \neq y
\end{align*}
\end{solution}



$\exists x\in X \suchthat P(x)$ is known as an \textbf{existential statement}. $X$ is known as the \textbf{domain}.

The quantifiers $\forall$ and $\exists$ are probably the most challenging of the notation. Do practice reading statements that include these symbols and checking that you understand their meaning.

\textbf{How to prove:} To prove a statement of the form $\forall x \in X \suchthat P(x)$’, start the proof with ‘Let $x \in X$.’ or ‘Suppose $x \in X$ is given.’ to address the quantifier with an arbitrary $x$; provided no other assumptions about $x$ are made during the course of proving $P(x)$, this will prove the statement for all $x \in X$. 

To prove a statement of the form $\exists x \in X \suchthat P(x)$, there is not such a clear steer about how to continue: you may need to show the existence of an $x$ with the right properties; you may need to demonstrate logically that such an $x$ must exist because of some earlier assumption, or it may be that you can show constructively how to find one; or you may be able to prove by contradiction, supposing that there is no such $x$ and consequently arriving at some inconsistency.

\begin{remark}
Read from left to right, and as new elements or statements are introduced they are allowed to depend on previously introduced elements but cannot depend on things that are yet to be mentioned.
\end{remark}

\begin{remark}
To avoid confusion, it is a good idea to keep to the convention that the quantifiers come first, before any statement to which they relate.
\end{remark}
\pagebreak

\section{Proofs}
\subsection{Direct Proof}
A direct proof of $P \implies Q$ is a series of valid arguments that start with the hypothesis $P$ and end with the conclusion $Q$. It may be that we can start from $P$ and work directly to $Q$, or it may be that we make use of $P$ along the way.

\subsection{Proof by Contrapositive}
To prove $P \implies Q$, we can instead prove $\lnot Q \implies \lnot P$.

\subsection{Proof by Mathematical Induction}
The following principle is sometimes quoted as a theorem, although it follows directly from our definition of the natural numbers.
\begin{thrm}{Principle of Induction}{}
Let $P(n)$ be a family of statements indexed by the natural numbers. Suppose that 
\begin{enumerate}[label=(\roman*)]
\item $P(1)$ is true and
\item for all $m \in \NN$, if $P(m)$ is true then $P(m+1)$ is also true.
\end{enumerate}
 Then $P(n)$ is true for all $n \in \NN$.
\end{thrm}

Using logic notation, this is written as
\[ \{P(1) \land (\forall n \in \ZZ^+) [P(m) \implies P(m+1)]\} \implies (\forall n \in \ZZ^+)P(n) \] 

Induction is often visualised like toppling dominoes. The \textbf{inductive step} (ii) corresponds to placing each domino sufficiently close that it will be hit when the previous one falls over, and the initial step, (i) -- known as the \textbf{base case} -- corresponds to knocking over the first one.

\begin{exmp}{}{}
Prove that for any $n \in \NN$,
\[ \sum_{k=1}^n k = \frac{n(n+1)}{2} \]
\end{exmp}

\begin{proof}
Clearly $P(1)$ holds because for $n=1$, the sum on the LHS is 1 and the expression on the RHS is also 1.

Now suppose $P(n)$ holds. Then
\begin{align*}
\sum_{k=1}^{n+1} k &= \sum_{k=1}^n k + (n+1) \\
&= \frac{n(n+1)}{2} + (n+1) \\
&= \frac{(n+1)(n+2)}{2}
\end{align*}
which is exactly the statement $P(n+1)$. So by induction, $P(n)$ is true for all $n \in \NN$.
\end{proof}

A corollary\footnote{an extension of, or a consequence of, a theorem or proposition; a corollary is generally not such a major result as the theorem or proposition itself.} of induction is if the family of statements holds for $n \ge N$, rather than necessarily $n \ge 0$:

\begin{corollary}
Let $N$ be an integer and let $P(n)$ be a family of statements indexed by integers $n \ge N$. Suppose that 
\begin{enumerate}[label=(\roman*)]
\item $P(N)$ is true and
\item for any $n \ge N$, if $P(n)$ is true then $P(n+1)$ is also true. 
\end{enumerate}
Then $P(n)$ is true for all $n \ge N$.
\end{corollary}

\begin{proof}
This follows directly by applying the above theorem to the statement $Q(n) = P(n+N)$ for $n \in N$.
\end{proof}

Another variant on induction is when the inductive step relies on some earlier case(s) but not necessarily the immediately previous case. This is sometimes \textbf{strong induction}:

\begin{thrm}{Strong Form of Induction}{}
Let $P(n)$ be a family of statements indexed by the natural numbers. Suppose that
\begin{enumerate}[label=(\roman*)]
\item $P(1)$ is true and
\item for all $m \in \NN$, if for integers $k$ with $1 \le k \le m$, $P(k)$ is true then $P(m+1)$ is true.
\end{enumerate}
Then $P(n)$ is true for all $n \in \NN$.
\end{thrm}

Using logic notation, this is written as
\[ \{P(1) \land (\forall m \in \ZZ^+) [P(1) \land P(2) \land \cdots \land P(m) \implies P(m+1)]\} \implies (\forall n \in \ZZ^+)P(n) \]

\begin{proof}
We can this it to an instance of ``normal" induction by defining a related family of statements $Q(n)$. 

Let $Q(n)$ be the statement ``$P(k)$ holds for $k=0,1,\dots,n$". Then the conditions for the strong form are equivalent to 
\begin{enumerate}[label=(\roman*)]
\item $Q(0)$ holds and 
\item for any $n$, if $Q(n)$ is true then $Q(n+1)$ is also true.
\end{enumerate}
It follows by induction that $Q(n)$ holds for all $n$, and hence $P(n)$ holds for all $n$.
\end{proof}

The following example illustrates how the strong form of induction can be useful:

\begin{exmp}{Fundamental Theorem of Arithmetic}{}
Every natural number greater than 1 may be expressed as a product of one or more prime numbers.
\end{exmp}

\begin{proof}
Let $P(n)$ be the statement that $n$ may be expressed as a product of prime numbers. 

Clearly $P(2)$ holds, since $2$ is itself prime. 

Let $n \ge 2$ be a natural number and suppose that $P(m)$ holds for all $m<n$.

\begin{itemize}
\item If $n$ is prime then it is trivially the product of the single prime number $n$. 

\item If $n$ is not prime, then there must exist some $r, s > 1$ such that $n = rs$. By the inductive hypothesis, each of $r$ and $s$ can be written as a product of primes, and therefore $n = rs$ is also a product of primes.
\end{itemize}

Thus, whether $n$ is prime or not, we have have that $P(n)$ holds. By strong induction, $P(n)$ is true for all natural numbers. That is, every natural number greater than 1 may be expressed as a product of one or more primes.
\end{proof}

Cauchy induction
\pagebreak

\begin{prbm}[FM/TJC/2023]
Prove by mathematical induction, for $n \ge 2$,
\[ \sqrt[n]{n}<2 - \frac{1}{n}. \]
\end{prbm}

\begin{proof}
Let $P(n)$ be the proposition that $\sqrt[n]{n}<2 - \dfrac{1}{n}$ for $n \ge 2$.

When $n=2$, $\sqrt{2} <2-\dfrac{1}{2}=1.5$ which is true. Hence $P(2)$ is true.

Assume $P(k)$ is true for $k \ge 2, k \in \ZZ^+$, i.e.
\[ \sqrt[k]{k}<2 - \dfrac{1}{k} \implies k<\brac{2-\frac{1}{k}}^k \]

We want to prove that $P(k+1)$ is true, i.e.
\[ k+1<\brac{2-\frac{1}{k+1}}^{k+1} \]

Since $k>2$, we have 
\begin{align*}
\brac{2-\frac{1}{k+1}}^{k+1}
&> \brac{2-\frac{1}{k}}^{k+1} \quad \because k>2 \\
&= \brac{2-\frac{1}{k}}^k\brac{2-\frac{1}{k}} \\
&> k\brac{2-\frac{1}{k}} \quad \text{by inductive hypothesis} \\
&= 2k-1 = k+k-1 > k-1 \because k>2
\end{align*}
Hence $P(k+1)$ is true.

Since $P(2)$ is true and $P(k)\implies P(k+1)$, by mathematical induction $P(n)$ is true.
\end{proof}
\pagebreak

\begin{prbm}
Prove that for all integers $n \ge 3$, 
\[ \brac{1+\frac{1}{n}}^n<n \]
\end{prbm}

\begin{proof}
Suppose for an integer $k$, we have 
\[ \brac{1+\frac{1}{k}}^k<k \]
Then
\[ \brac{1+\frac{1}{k}}^k\brac{1+\frac{1}{k}} = \brac{1+\frac{1}{k}}^{k+1}<k\brac{1+\frac{1}{k}} = k+1  \]
Note 
\[ \brac{1+\frac{1}{k}}^{k+1} > \brac{1+\frac{1}{k+1}}^{k+1} \quad \text{since } k<k+1 \iff \frac{1}{k}>\frac{1}{k+1} \]
The rest of the proof follows easily.
\end{proof}

\subsection{Disproof by Counterexample}
Providing a counterexample is the best method for refuting, or dispoving, a conjecture. 

In seeking counterexamples, it is a good idea to keep the cases you consider simple, rather than searching randomly. It is often helpful to consider ``extreme" cases; for example, something is zero, a set is empty, or a function is constant.

The counterexample must make the hypothesis a true statement, and the conclusion a false statement.

\subsection{Proof by Contradiction}
To prove $P \implies Q$ by contradiction, we suppose that $Q$ is not true and show through some logical reasoning (making use of the hypothesis $P$) that this leads to a contradiction or inconsistency. We may arrive at something that contradicts the hypotheses $P$, or something that contradicts the initial supposition that $Q$ is not true, or we may arrive at something that we know to be universally false.

\subsection{Proof of Existence}
\subsection{Proof by Construction}

\begin{exmp}{Irrationality of $\sqrt{2}$}{}
Prove that $\sqrt{2}$ is irrational.
\end{exmp}
\begin{proof}
We prove by contradiction. Suppose otherwise, that $\sqrt{2}$ is rational. Using the definition of rational numbers, we can write it as $\sqrt{2} = \dfrac{a}{b}$ for some $a,b\in\ZZ,b\neq 0$. 

We also assume that $\dfrac{a}{b}$ is simplified to lowest terms, since that can obviously be done with any fraction. Notice that in order for $\dfrac{a}{b}$ to be in simplest terms, both $a$ and $b$ cannot be even; one or both must be odd, otherwise we could simplify the fraction further.

Squaring both sides gives us
\[ a^2 = 2b^2. \]
Since RHS is even, LHS must also be even. Hence it follows that $a$ is even. Let $a=2k$ where $k\in\ZZ$. Substituting $a = 2k$ into the above equation and simplifying it gives us
\[ b^2=2k^2. \]
This means that $b^2$ is even, from which follows again that $b$ is even. 

This is a contradiction, as we started out assuming that $\dfrac{a}{b}$ was simplified to lowest terms, and now it turns out that $a$ and $b$ both would be even. Hence proven.
\end{proof}

\chapter{Set Theory}
\section{Basics}
\subsection{Notation}
You should, by now, be familiar with the following definitions and notation:
\begin{itemize}
\item A \vocab{set} $S$ can be loosely defined as a collection of objects.

\item For a set $S$, we write $x \in S$ to mean that $x$ is an \vocab{element} of $S$, and $x \notin S$ if otherwise.

\item A set can be defined in terms of some property $P(x)$ that the elements $x \in S$ satisfy, denoted by 
\[ \{x \in S \mid P(x)\} \]

\item Some basic sets (of numbers) you should be familiar with:
\begin{itemize}
\item $\NN=\{0,1,2,3,\dots\}$ denotes the natural numbers (non-negative integers).
\item $\ZZ=\{\dots,-2,-1,0,1,2,\dots\}$ denotes the integers.
\item $\QQ=\{\frac{p}{q} \mid p,q\in\ZZ, q\neq0\}$ denotes the rational numbers.
\item $\RR$ denotes the real numbers, which can be expressed in terms of decimal expansion.
\item $\CC=\{x+yi \mid x,y\in\RR\}$ denotes the of complex numbers.
\end{itemize}

\item The \vocab{empty set} is the set with no elements, denoted by $\emptyset$.

\item $A$ is a \vocab{subset} of $B$ if every element of $A$ is in $B$, denoted by $A \subseteq B$.
\[ A \subseteq B \iff \forall x, x\in A \implies x\in B \]

$\subseteq$ is transitive, i.e. if $A \subseteq B$ and $B \subseteq C$, then $A \subseteq C$.
\begin{proof}
Let $x\in A$. 
Since $A \subseteq B$ and $x\in A$, $x\in B$. 
Since $B \subseteq C$ and $x\in B$, $x\in C$. 
Hence $A \subseteq C$.
\end{proof}

$A$ is a \vocab{proper subset} of $B$ if $A \subseteq B$ and $A \neq B$, denoted by $A \subset B$.

Using this definition, we have the relationship 
\[ \NN \subset \ZZ \subset \QQ \subset \RR \]

\item $A$ and $B$ are \vocab{equal} if and only if they contain the same elements, denoted by $A=B$.

To prove that $A$ and $B$ are equal, we simply need to prove that $A \subseteq B$ and $A \subseteq B$.

\begin{proof}
We have 
\begin{align*}
A = B &\iff (\forall x)[x \in A \iff x \in B] \\
&\iff (\forall x)[(x \in A \implies x \in B) \land (x \in B \implies x \in A)] \\
&\iff \{(\forall x)[x \in A \implies x \in B]\} \land {(\forall x)[x \in B \implies x \in A)]} \\
&\iff (A \subseteq B) \land (B \subseteq A)
\end{align*}
\end{proof}

\item Some frequently occurring subsets of the real numbers are known as \vocab{intervals}, which can be visualised as sections of the real line:
\begin{itemize}
\item Open interval
\[ (a,b) = \{x\in\RR \mid a<x<b\} \]
\item Closed interval
\[ [a,b] = \{x\in\RR \mid a\le x<b\} \]
\item Half open interval
\[ (a,b] = \{x\in\RR \mid a<x\le b\} \]
\end{itemize}

\item The \vocab{power set} $\mathcal{P}(A)$ of $A$ is the set of all subsets of $A$ (including the set itself and the empty set).

\item An \vocab{ordered pair} is denoted by $(a,b)$, where the order of the elements matters. Two pairs $(a_1,b_1)$ and $(a_2,b_2)$ are equal if and only if $a_1=a_2$ and $b_1=b_2$. 

Similarly, we have ordered triples $(a,b,c)$, quadruples $(a,b,c,d)$ and so on. If there are $n$ elements it is called an $n$-tuple.

\item The \vocab{Cartesian product} of sets $A$ and $B$, denoted by $A \times B$, is the set of all ordered pairs with the first element of the pair coming from $A$ and the
second from $B$:
\begin{equation}
A \times B = \{(a,b) \mid a \in A, b \in B\}
\end{equation}
If $A = B$, we write $A \times A$ as $A^2$. Note that the case where $A = B = \RR$ is a particularly important one as $\RR^2$ represents the two-dimensional real plane.

More generally, we define $A_1 \times A_2 \times \cdots \times A_n$ to be the set of all ordered $n$-tuples $(a_1, a_2, \dots, a_n)$, where $a_i \in A_i$ for $1 \le i \le n$. If all the A$_i$ are the same, we write the product as $A^n$.

\begin{exmp}{}{}
$\RR^2$ is the Euclidean plane, $\RR^3$ is the Euclidean space, and $\RR^n$ is the $n$-dimensional Euclidean space.
\begin{align*}
\RR \times \RR = \RR^2 &= \{(x,y) \mid x,y \in \RR\} \\
\RR \times \RR \times \RR = \RR^3 &= \{(x,y,z) \mid x,y,z \in \RR\} \\
\RR^n &= \{(x_1,x_2,\dots,x_n) \mid x_1,x_2,\dots,x_n \in \RR\}
\end{align*}
\end{exmp}
\end{itemize}

\subsection{Algebra of Sets}
Given $A \subset S$ and $B \subset S$.
\begin{itemize}
\item The \vocab{union} $A \cup B$ is the set consisting of elements that are in $A$ or $B$ (or both):
\[ A\cup B=\{x \in S \mid x\in A \lor x\in B\} \]

\item The \vocab{intersection} $A \cap B$ is the set consisting of elements that are in both $A$ and $B$:
\[ A\cap B=\{x \in S \mid x\in A \land x\in B\} \]

$A$ and $B$ are \vocab{disjoint} if both sets have no element in common:
\[ A\cap B = \emptyset \]

More generally, we can take unions and intersections of arbitrary numbers of sets, even infinitely many. If we have a family of subsets $\{A_i \mid i \in I\}$, where $I$ is an \textbf{indexing set}, we write
\[ \bigcup_{i\in I} A_i = \{x \mid \exists i\in I\,(x\in A_i)\} \]
and
\[ \bigcap_{i\in I} A_i = \{x \mid \forall i\in I\,(x\in A_i)\} \] 

\item The \vocab{complement} of $A$, denoted by $A^c$ or $A^\prime$, is the set containing elements that are not in A:
\[ A^c = \{x \in S \mid x \notin A\} \]

\item The \vocab{set difference}, or complement of $B$ in $A$, written $A\setminus B$, is the subset consisting of those elements that are in $A$ and not in $B$:
\[ A\setminus B = \{x \in A \mid x \notin B\} \]
Note that $A\setminus B = A \cap B^c$.
\end{itemize}

\begin{proposition}[Double Inclusion]
Let $A\subset S$ and $B\subset S$. Then 
\begin{equation}
A = B \iff A \subseteq B \text{ and } B \subseteq A
\end{equation}
\end{proposition}
\begin{proof} We prove both directions.

\textbf{Forward direction:}

If $A = B$, then every element in $A$ is an element in $B$, so certainly $A \subseteq B$, and
similarly $B \subseteq A$. 

\textbf{Backward direction:}

Suppose $A \subseteq B$, and $B \subseteq A$. Then for every element $x \in S$, if $x \in A$ then $A \subseteq B$ implies that $x \in B$, and if $x \notin A$ then $B \subseteq A$ means $x \notin B$. So $x \in A$ if and only if $x \in B$, and therefore $A = B$.
\end{proof}

\begin{proposition}[Distributive Laws]
Let $A\subset S$, $B\subset S$ and $C\subset S$. Then
\begin{equation}
(A\cup B)\cap C = (A\cap C)\cup(B\cap C)
\end{equation}
\begin{equation}
(A\cap B)\cap C = (A\cup C)\cap(B\cup C)
\end{equation}
\end{proposition}
\begin{proof}
For the first one, suppose $x$ is in the LHS, that is $x \in A\cup(B \cap C)$. This means that $x \in A$ or $x \in B \cap C$ (or both). Thus either $x \in A$ or $x$ is in both $B$ and $C$ (or $x$ is in all three sets). If $x \in A$ then $x \in A\cup B$ and $x \in A\cup C$, and therefore $x$ is in the RHS. If $x$ is in both $B$ and $C$ then similarly $x$ is in both $A\cup B$ and $A\cup C$. Thus every element of the LHS is in the RHS, which means we have shown $A \cup (B \cap C) \subseteq (A \cup B) \cap (A \cup C)$.

Conversely suppose that $x \in (A \cup B) \cap (A \cup C)$. Then $x$ is in both $A \cup B$ and $A\cup C$. Thus either $x \in A$ or, if $x \notin A$, then $x \in B$ and $x \in C$. Thus $x \in A\cup(B \cap C)$. Hence $(A \cup B) \cap (A \cup C) \subseteq A \cup (B \cap C)$.

By double inclusion, $(A \cup B) \cap (A \cup C) = A \cup (B \cap C)$.

The proof of the second one follows similarly and is left as an exercise.
\end{proof}

\begin{proposition}[De Morgan's Laws]
Let $A \subset S$ and $B \subset S$. Then
\begin{equation}
(A \cup B)^c = A^c \cap B^c
\end{equation}
\begin{equation}
(A \cap B)^c = A^c \cup B^c
\end{equation}
\end{proposition}
\begin{proof}
For the first one, suppose $x \in (A \cup B)^c$. Then $x$ is not in either $A$ or $B$. Thus $x \in A^c$ and $x \in B^c$, and therefore $x \in A^c \cap B^c$. 

Conversely, suppose $x \in A^c \cap B^c$. Then $x \notin A$ and $x \notin B$, so $x$ is in neither $A$ nor $B$, and therefore $x \in (A \cup B)^c$.

By double inclusion, the first result holds. The second result follows similarly and is left as an exercise.
\end{proof}

De Morgan’s laws extend naturally to any number of sets, so if $\{A_i \mid i \in I\}$ is a family of subsets of $S$, then
\[ \brac{\bigcap_{i\in I}A_i}^c = \bigcup_{i\in I}A_i^c \quad \text{and} \quad \brac{\bigcup_{i\in I}A_i}^c = \bigcap_{i\in I}A_i^c \]

\begin{exercise}{}{}
Prove the following:
\begin{enumerate}
\item $\brac{\bigcup_{i\in I}A_i}\cup B=\bigcup_{i\in I}(A_i\cup B)$
\item $\brac{\bigcap_{i\in I}A_i}\cup B=\bigcap_{i\in I}(A_i\cup B)$
\item $\brac{\bigcup_{i\in I}A_i}\cup\brac{\bigcup_{j\in J}B_j}=\bigcup_{(i,j)\in I\times J}(A_i\cup B_j)$
\item $\brac{\bigcap_{i\in I}A_i}\cup\brac{\bigcap_{j\in J}B_j}=\bigcap_{(i,j)\in I\times J}(A_i\cup B_j)$
\end{enumerate}
\end{exercise}

\begin{exercise}{}{}
Let $S\subset A\times B$. Express the set $A_S$ of all elements of $A$ which appear as the first entry in at least one of the elements in $S$.

($A_S$ here may be called the projection of $S$ onto $A$.)
\end{exercise}

\begin{prbm}
Let $A$ be the set of all complex polynomials in $n$ variables. Given a subset $T \subset A$, define the \textit{zeros} of $T$ as the set
\[ Z(T) = \{P=(a_1,\dots,a_n) \mid f(P)=0 \text{ for all } f \in T\} \]
A subset $Y \in \CC^n$ is called an algebraic set if there exists a subset $T \subset A$ such that $Y=Z(T)$.

Prove that the union of two algebraic sets is an algebraic set.
\end{prbm}
\begin{proof}
We would like to consider $T=\{f_1, f_2, \dots\}$ expressed as indexed sets $T=\{f_i\}$. Then $Z(T)$ can also be expressed as $\{P \mid \forall i, f_i(P)=0\}$.

Suppose that we have two algebraic sets $X$ and $Y$. Let $X=Z(S)$, $Y=Z(T)$ where $S,T$ are subsets of $A$ (basically, they are certain sets of polynomials). Then
\[ X=\{P \mid \forall f \in S, f(P)=0\} \]
\[ Y=\{P \mid \forall g \in T, g(P)=0\} \]

We imagine that for $P\in X\cap Y$, we have $f(P)=0$ or $g(P)=0$. Hence we consider the set of polynomials
\[ U=\{f\cdot g \mid f\in S, g\in T\} \]

For any $P\in X\cup Y$ and for any $fg\in U$ where $f\in S$ and $f\in g$, either $f(P)=0$ or $g(P)=0$, hence $fg(P)=0$ and thus $P\in Z(U)$.

On the other hand if $P\in Z(U)$, suppose otherwise that $P$ is not in $X\cup Y$, then $P$ is neither in $X$ nor in $Y$. This means that there exists $f\in S,g\in T$ such that $f(P)\neq0$ and $g(P)\neq0$, hence $fg(P)\neq0$. This is a contradiction as $P\in Z(U)$ implies $fg(P)=0$. Hence we have $X\cup Y=Z(U)$ and thus $X\cup Y$ is an algebraic set.

Now the other direction is simpler and can actually be generalised: The intersection of arbitrarily many algebraic sets is algebraic. 

The basic result is that if $X=Z(S)$ and $Y=Z(T)$ then $X\cap Y=Z(S\cup T)$. 

This is the beginning of a subject called algebraic geometry, which discusses about the geometry of algebraic surfaces, namely those kinds of surfaces defined by zeroes of polynomials (conic sections for example).
\end{proof}
\pagebreak

\subsection{Cardinality}
Informally, the \vocab{cardinality} of $S$, denoted $|S|$, is a measure of its ``size".

We will be able to give a nicer definition of cardinality later, once we have discussed bijections, but the following provides a recursive definition of the cardinality for a finite set:

\begin{defn}{Finiteness and the cardinality of a finite set}{}
The empty set $\emptyset$ is
finite with $|\emptyset|=0$. $S$ is finite with $|S|= n+1$, if there exists $s \in S$ such that $|S\setminus \{s\}| = n$ for some $n \in \NN$. We call $|S|$ the \vocab{cardinality} of $S$. Any set that is not finite is said to be infinite.
\end{defn}

It is not hard to see that this means that if $S = \{x_1,x_2,\dots,x_n\}$, and $x_i \neq x_j$ whenever $i \neq j$, then $|S| = n$. Conversely, if $|S| = n$ then $S$ is a set with $n$ elements.

\begin{proposition}
Let $A$ and $B$ be finite sets. Then $|A \cup B| = |A| + |B| - |A \cap B|$.
\end{proposition}

\begin{proof}
The proof is left as an exercise.
\end{proof}

\begin{proposition}[Subsets of a finite set]
If a set $A$ is finite with $|A| = n$, then its power set has $|\mathcal{P}(A)| = 2^n$.
\end{proposition}

\begin{proof}
We use induction. For the initial step, note that if $|A| = 0$ then $A = \emptyset$ has no elements, so there is a single subset $\emptyset$, and therefore $|\mathcal{P}(A)| = 1 = 2^0$.

Now suppose that $n \ge 0$ and that $|P(S)| = 2^n$ for any set S with $|S| = n$. Let $A$ be any set with $|A| = n+1$. By definition, this means that there is an element $a$ and a set $A_0 = A\setminus\{a\}$ with $|A_0| = n$. Any subset of A must either contain the element a or not, so we can partition $\mathcal{P}(A) = P(A_0) \cup \{S \cup \{a\} \mid S \in P(A_0)\}$. These two sets are disjoint, and each of them has cardinality $|P(A_0)| = 2^n$ by the inductive hypothesis. Hence $|\mathcal{P}(A)| = 2^n + 2^n = 2^{n+1}$.

Thus, by induction, the result holds for all $n$.
\end{proof}

Another way to see this is through combinatorics: Consider the process of creating a subset. We can do this systematically by going through each of the $|A|$ elements in $A$ and making the yes/no decision whether to put it in the subset. Since there are $|A|$ such choices, that yields $2^{|A|}$ different combinations of elements and therefore $2^{|A|}$ different subsets.

\pagebreak

\section{Relations}
\subsection{Definition}
A relation is a set of ordered pairs.

\begin{defn}{Relation}{}
$R$ is a \vocab{relation} between $A$ and $B$ if and only if $R$ is a subset of the Cartesian product $A \times B$, i.e. $R \subseteq A \times B$.

$a \in A$ and $b \in B$ are \textbf{related} if $(a,b) \in R$, denoted by $a R b$.
\end{defn}

Visually speaking, a relation is uniquely determined by a simple bipartite graph over $A$ and $B$. On the bipartite graph, this is usually represented by an edge between $a$ and $b$.

\begin{defn}{Binary relation}{}
A \vocab{binary relation} in $A$ is a relation between $A$ and itself, i.e. $R \subseteq A \times A$.
\end{defn}

$A$ and $B$ are the \vocab{domain} and \vocab{range} of $R$ respectively, denoted by $\dom R$ and $\ran R$ respectively, if and only if $A \times B$ is the smallest Cartesian product of which $R$ is a subset.

\begin{exmp}{}{}
$R = \{(1,a), (1,b), (2,b), (3,b)\}$, then 
\begin{itemize}
\item $\dom R = \{1,2,3\}$
\item $\ran R = \{a,b\}$
\end{itemize}
\end{exmp}

In many cases we do not actually use $R$ to write the relation because there is some other conventional notation:

\begin{exmp}{}{}
\begin{itemize}
\item The ``less than or equal to" relation $\le$ on the set of real numbers is $\{(x,y) \in \RR^2 \mid x \le y\}$. We write $x \le y$ if $(x,y)$ is in this set.
\item The ``divides" relation $\mid$ on $\NN$ is $\{(m,n) \in \NN^2: m \text{ divides } n\}$. We write $m \mid n$ if $(m,n)$ is in this set.
\item For a set S, the ``subset" relation $\subseteq$ on $\mathcal{P}(S)$ is $\{(A,B) \in \mathcal{P}(S)^2 \mid A \subseteq B\}$. We write $A \subseteq B$ if $(A,B)$ is in this set.
\end{itemize}
\end{exmp}
\pagebreak

\subsection{Properties of relations}
Let $A$ be a set, $R$ a relation on $A$, and $x,y,z \in A$. We say that
\begin{itemize}
\item $R$ is \textbf{reflexive} if $xRx$ for all $x$ in $A$.
\item $R$ is \textbf{symmetric} if whenever $xRy$ then $yRx$.
\item $R$ is \textbf{anti-symmetric} if whenever $xRy$ and $yRx$ then $x = y$.
\item $R$ is \textbf{transitive} if whenever $xRy$ and $yRz$ then $xRz$.
\end{itemize}

\begin{exmp}{Less than or equal to}{}
The relation $\le$ on $R$ is reflexive, anti-symmetric, and transitive, but not symmetric. 
\end{exmp}

More generally, any relation on $A$ that is reflexive, anti-symmetric, and transitive is called a \textbf{partial order}, denoted by $\le$. It is called a \textbf{total order} if for every $x, y \in A$, either $xRy$ or $yRx$ (or both).

\begin{exmp}{Less than}{}
The relation $<$ on $R$ is not reflexive, symmetric, or anti-symmetric, but it is transitive.
\end{exmp}

\begin{exmp}{Not equal to}{}
The relation $\neq$ on $R$ is not reflexive, anti-symmetric or transitive, but it is symmetric.
\end{exmp}

\begin{exmp}{Congruence modulo $n$}{}
Let $n \ge 2$ be an integer, and define $R$ on $\ZZ$ by saying $aRb$ if and only if $a-b$ is a multiple of $n$. Then $R$ is reflexive, symmetric and transitive.
\end{exmp}
\begin{proof} \
\begin{itemize}
\item Reflexivity: For any $a \in \ZZ$ we have $aRa$ as 0 is a multiple of $n$.
\item Symmetry: If $aRb$ then $a-b=kn$ for some integer $k$. So $b-a=-kn$, and hence $bRa$.
\item Transitivity: If $aRb$ and $bRc$ then $a-b=kn$ and $b-c=ln$ for integers $k,l$. So then $a-c=(a-b)+(b-c)=(k+l)n$, and hence $aRc$.
\end{itemize}
\end{proof}
\pagebreak

\subsection{Equivalence relations, equivalence classes, and partitions}
One important type of relation is an equivalence relation. An equivalence relation is a way of saying two objects are, in some particular sense, ``the same".

\begin{defn}{Equivalence relation}{}
A binary relation $R$ on $A$ is an \vocab{equivalence relation} if it is reflexive, symmetric and transitive. If $R$ is an equivalence relation, we denote it by $\sim$.
\end{defn}

\begin{remark}
We use the symbol $\sim$ to denote the equivalence relation $R$ in $A \times A$, and whenever $(a,b)\in R$ we denote $a \sim b$.
\end{remark}

An equivalence relation provides a way of grouping together elements that can be viewed as being the same:

\begin{defn}{Equivalence class}{}
Given an equivalence relation $\sim$ on a set $A$, and given $x \in A$, the \vocab{equivalence class} of $x$, denoted $[x]$, is the subset
\[ [x] = \{y \in A \mid y \sim x\} \]
\end{defn}

\begin{exmp}{Congruence modulo $n$}{}
For the equivalence relation of congruence modulo $n$, the equivalence class of 1 is the set $1 = \{\dots, -n+1, 1, n+1, 2n+1, \dots\}$; that is, all the integers that are congruent to 1 modulo $n$.
\end{exmp}

Properties of equivalence classes:
\begin{itemize}
\item Every two equivalence classes are disjoint
\item The union of equivalence classes form the entire set
\end{itemize}

You can translate these properties into the point of view from the elements: Every element belongs to one and only one equivalence class.
\begin{itemize}
\item No element belongs to two distinct classes
\item All elements belong to an equivalence class
\end{itemize}

\begin{defn}{Set of equivalence classes}{}
The \vocab{set of equivalence classes} (quotient sets) are the set of all equivalence classes, denoted by $A/\sim$.
\end{defn}

Grouping the elements of a set into equivalence classes provides a partition of the set, which we define as follows:

\begin{defn}{Partition}{}
A \vocab{partition} of a set $A$ is a collection of subsets $\{A_i \subseteq A \mid i \in I\}$, where $I$ is an indexing set, with the property that
\begin{enumerate}[label=(\roman*)]
\item $A_i \neq \emptyset$ for all $i \in I$ (that is, all the subsets are non-empty),
\item $\bigcup_{i\in I} Ai = A$ (that is, every member of $A$ lies in one of the subsets),
\item $A_i \cap A_j = \emptyset$ for every $i \neq j$ (that is, the subsets are disjoint).
\end{enumerate}
The subsets are called the parts of the partition.
\end{defn}

\begin{exmp}{Odd and even natural numbers}{}
$\{\{n \in \NN \mid n \text{ is divisible by } 2\}, \{n \in \NN \mid n+1 \text{ is divisible by } 2\}\}$ forms a partition of the natural numbers, into evens and odds.
\end{exmp}
\pagebreak

\begin{prbm}[Modular Arithmetic]
Define the ring of integers modulo $n$:
\[ \ZZ/n\ZZ = \ZZ/\sim \text{ where } x \sim y \iff x-y \in n\ZZ. \]
The equivalence classes are called congruence classes modulo $n$.
\begin{enumerate}[label=(\alph*)]
\item Define the sum of two congruence classes modulo $n$, $[x], [y] \in \ZZ/n\ZZ$, by
\[ [x] + [y] = [x + y] \]
Show that the above definition is well-defined.
\item Define the product of two congruence classes modulo $n$ and show that such a definition is well-defined.
\end{enumerate} 
\end{prbm}

\begin{solution} \
\begin{enumerate}[label=(\alph*)]
\item We often define such concepts by considering the \textbf{representatives} of the equivalence classes.

For example, here we define $[x]+[y]=[x+y]$ for two elements $[x]$ and $[y]$ in $\ZZ/n\ZZ$. So what we know here are the classes $[x]$ and $[y]$. But what exactly are $x$ and $y$? They are just some element in the equivalence classes that was arbitrarily picked out. We then perform the sum $x+y$, and consequently, we used this to point towards the class $[x+y]$. 

However, $x$ and $y$ are arbitrarily picked. We want to show that, regardless of which representatives are chosen from  the equivalence classes $[x]$ and $[y]$, we will always obtain the same result.

In the definition itself, we have defined that, for the two representatives $x$ and $y$ we define $[x]+[y]=[x+y]$. So now, let's say that we take two other arbitrary representatives, $x^\prime\in [x]$ and $y^\prime\in [y]$. 
Then by definition, we have
\[ [x]+[y]=[x^\prime+y^\prime] \]

Thus, our goal is to show that $x^\prime+y^\prime]=[x+y]$. 
This expression means that the two sides of the equation are referring to the same equivalence class.
Therefore, the expression above is completely equivalent to the condition.
\[ x^\prime+y^\prime \sim x+y \]

We then check that this final expression is indeed true:
Since $x^\prime\in [x]$ and $y^\prime\in [y]$, we have 
\begin{align*}
&x^\prime\sim x \text{ and } y^\prime\sim y \\
&\implies x^\prime-x, y^\prime-y\in n\ZZ \\
&\implies (x^\prime+y^\prime)-(x+y)=(x^\prime-x)+(y^\prime-y)\in n\ZZ
\end{align*}

\item The product of two congruence classes is defined by
\[ [x][y]=[xy] \]

For any other representatives $x^\prime$, $y^\prime$ we have
\begin{align*}
&x^\prime y^\prime-xy \\
&=x^\prime y^\prime-xy^\prime+xy^\prime-xy \\
&=(x^\prime-x)y^\prime+x(y^\prime-y) \in n\ZZ
\end{align*}

Thus $[x^\prime y^\prime]=[xy]$ and the product is well-defined.
\end{enumerate}
\end{solution}
\pagebreak

\begin{prbm}
Let $A = \RR$ and for any $x, y \in A$, $x \sim y$ if and only if $x-y \in \ZZ$. For any two equivalence classes $[x], [y] \in A/\sim$, define
\[ [x] + [y] = [x + y] \text{ and } -[x] = [-x] \]
\begin{enumerate}[label=(\alph*)]
\item Show that the above definitions are well-defined.
\item Find a one-to-one correspondence $\phi:X \to Y$ between $X = A/\sim$ and $Y:|z| = 1$, i.e. the unit circle in $\CC$, such that for any $[x_1], [x_2] \in X$ we have
\[ \phi([x_1])\phi([x_2]) = \phi([x_1 + x_2]) \]
\item Show that for any $[x] \in X$,
\[ \phi(-[x]) = \phi([x])^{-1} \]
\end{enumerate}
\end{prbm}

\begin{solution} \ 
\begin{enumerate}[label=(\alph*)]
\item 
\[ (x^\prime+y^\prime)-(x+y)=(x^\prime-x)+(y^\prime-y)\in \ZZ \]
Thus $[x^\prime+y^\prime]=[x+y]$

\[ (-x^\prime)-(-x)=-(x^\prime-x)\in \ZZ \]
Thus $[-x^\prime]=[-x]$.

\item Complex numbers in the polar form: $z=re^{i\theta}$

Then the correspondence is given by $\phi([x])=e^{2\pi ix}$
\[ [x]=[y] \iff x-y\in \ZZ \iff e^{2\pi i(x-y)}=1 \iff e^{2\pi ix}=e^{2\pi iy} \]
Hence this is a bijection.

Before that, we also need to show that $\phi$ is well-defined, which is almost the same as the above.

If we choose another representative $x^\prime$ then
\[ \phi([x])=e^{2\pi ix^\prime} = e^{2\pi ix}\cdot e^{2\pi i(x^\prime-x)} = e^{2\pi ix} \]

\item You can either refer to the specific correspondence $\phi([x])=e^{2\pi ix}$ or use its properties.
\[ \phi(-[x])\phi([x]) = \phi([-x])\phi([x]) = \phi([-x+x]) = \phi([0]) = 1 \]
\end{enumerate}
\end{solution}
\pagebreak

\begin{prbm}[Set of Rational Numbers] 
Let $\ZZ$ be the set of integers, and let $\ZZ^\ast$ be the set of nonzero integers. We define
\[ \QQ = \{(a, b) \mid a \in \ZZ, b \in \ZZ^\ast\}/\sim \]
where
\[ (a, b) \sim (c, d) \iff ad = bc.\]
Let $\dfrac{a}{b}$ denote the equivalence class for $(a,b)$. Such an equivalence class is called a rational number.
\begin{enumerate}[label=(\alph*)]
\item For any two rational numbers $\dfrac{a}{b}$ and $\dfrac{c}{d}$, their sum is determined by
\[ \frac{a}{b}+\frac{c}{d}=\frac{ad+bc}{bd} \]
Show that the above definition is well-defined.

\item Define the product of two rational numbers and show that such a definition is well-defined.

\item Prove that for every equivalence class $\dfrac{a}{b} \in \QQ$, there exists a unique integer pair $(p,q)$ satisfying the following properties:
\[ q>0, (p,q) = 1 \text{ and } (p,q) \in \frac{a}{b}.\]

\item Using the partial order of $\ZZ$, define the partial order of $\QQ$.
\end{enumerate}
\end{prbm}

\begin{solution} \ 
\begin{enumerate}[label=(\alph*)]
\item For this problem, we are dealing with a ``hidden" equivalence class.

The expressions $\frac{a}{b}$ and $\frac{c}{d}$ themselves are derived from their representatives $(a,b)$ and $(c,d)$.

So suppose that we choose other representatives $(a^\prime,b^\prime)$ and $(c^\prime,d^\prime)$, then the sum would be
\[ \frac{a^\prime}{b^\prime} + \frac{c^\prime}{d^\prime} = \frac{a^\prime d^\prime + b^\prime c^\prime}{b^\prime d^\prime} \]

We now have to show that $\frac{ad+bc}{bd} = \frac{a^\prime d^\prime + b^\prime c^\prime}{b^\prime d^\prime}$:
\begin{align*}
\frac{ad+bc}{bd} 
&\iff (ad+bc,bd) \sim (a^\prime d^\prime+b^\prime c^\prime,b^\prime d^\prime) \\
&\iff (ad+bc)b^\prime d^\prime =(a^\prime d^\prime+b^\prime c^\prime)bd
\end{align*}

\begin{align*}
\frac{a}{b} &= \frac{a^\prime}{b^\prime} \\
(a,b) &\sim (a^\prime,b^\prime) \\
ab^\prime &= a^\prime b
\end{align*}

Hence
\begin{align*}
(ad+bc)b^\prime d^\prime
&= ab^\prime dd^\prime + bb^\prime cd^\prime \\
&= a^\prime bdd^\prime + bb^\prime c^\prime d \\
&= (a^\prime d^\prime + b^\prime c^\prime )bd
\end{align*}

\item The definition would be $\frac{a}{b} \cdot \frac{c}{d} = \frac{ac}{bd}$.

This is actually a lot simpler to check.
\[ a^\prime c^\prime bd=(a^\prime b)(c^\prime d)=(ab^\prime)(cd^\prime)=acb^\prime d^\prime \]
Hence $\frac{a^\prime c^\prime}{b^\prime d^\prime} = \frac{ac}{bd}$.

\item We basically try to do this step by step as we would in simplifying fractions.

First pick $b$ to be positive, otherwise we swap $a$ and $b$ with $-a$ and $-b$.

Then simplify the common factors. For this one we let $(a,b)=d$, and $a=dp,b=dq$. Then $(p,q)$ is the pair that we need

\item In order to define the partial order we need to account for whether the denominators are negative.

$\frac{a}{b} \le \frac{c}{d}$, and if $b,d>0$ then we can safely draw a connection to the expression $ad \le bc$

In order to show that this does in fact give a partial order we check that
\begin{enumerate}
\item 1: $ab \le ab$ and hence $\frac{a}{b} \le \frac{a}{b}$

\item 2: If $\frac{a}{b} \le \frac{c}{d}$ and $\frac{c}{d} \le \frac{a}{b}$, then $ad \le bc$ and $bc \le ad$, hence $ad=bc$ and thus $\frac{a}{b}=\frac{c}{d}$

\item 3: This is trickier due to complications arising from inequalities and multiplication

If $\frac{a}{b}\le\frac{c}{d}$ and $\frac{c}{d}\le\frac{e}{f}$, note that $b,d,f>0$ and so $ad \le bc$ and $cf \le de$.

i) $e<0$, then $c<0$ and $a<0$, thus $-ad \ge -bc$, $-cf \ge -de$ and we have $acdf \ge bcde$

$af \le be (c<0,d>0)$

Thus $\frac{a}{b} \le \frac{e}{f}$

ii) $e \ge 0$ but $a<0$, then $af<0 \le be$ and thus $\frac{a}{b}<\frac{e}{f}$

iii) $a \ge 0$, then $c \ge 0$ and $e \ge 0$, and we have the ordinary case.
\end{enumerate}

Hence proven.
\end{enumerate}
\end{solution}
\pagebreak

\section{Functions}
\subsection{Definition}
\begin{defn}{Function}{}
Given two sets $X$ and $Y$, a \vocab{function} $f$ from $X$ to $Y$ is a mapping of every element of $X$ to some element of $Y$, denoted by $f:X\to Y$. 

We call $X$ and $Y$ the \textbf{domain} and \textbf{codomain} of $f$ respectively.
\end{defn}

\begin{remark}
The definition requires that a unique element of the codomain is assigned for every element of the domain. For example, for a function $f:\RR \to \RR$, the assignment $f(x)=\frac{1}{x}$ is not sufficient as it fails at $x=0$. Similarly, $f(x)=y$ where $y^2=x$ fails because $f(x)$ is undefined for $x<0$, and for $x>0$ it does not return a unique value; in such cases, we say the the function is \textbf{ill-defined}. We are interested in the opposite; functions that are \textbf{well-defined}.
\end{remark}

\begin{defn}{Image and pre-image}{}
Given a function $f:X \to Y$, the \vocab{image} (or range) of $f$ is
\[ f(X) = \{f(x) \mid x \in X\} \subseteq Y \]
More generally, given $A \subseteq X$, the image of $A$ under $f$ is
\[ f(A) = \{f(x) \mid x \in A\} \subseteq Y \]
Given $B \subseteq Y$, the \vocab{pre-image} of $B$ under $f$ is
\[ f^{-1}(B) = \{x \mid f(x) \in B\} \subseteq X \]
\end{defn}

\begin{remark}
Beware the potentially confusing notation: for $x \in X$, $f(x)$ is a single element of $Y$, but for $A \subseteq X$, $f(A)$ is a set (a subset of $Y$). Note also that $f^{-1}(B)$ should be read as ``the pre-image of $B$" and not as ``$f$-inverse of $B$"; the pre-image is defined even if no inverse function exists (in which case $f^{-1}$ on its own has no meaning; we discuss invertibility of a function below).
\end{remark}

If a function is defined on some larger domain than we care about, it may be helpful to restrict the domain:

\begin{defn}{Restriction}{}
Given a function $f:X \to Y$ and a subset $A \subseteq X$, the \vocab{restriction} of $f$ to $A$ is the map $f|_A:A \to Y$ defined by $f|_A(x) = f(x)$ for all $x \in A$.
\end{defn}

The restriction is almost the same function as the original $f$ -- just the domain has changed.

Another rather trivial but nevertheless important function is the identity map:

\begin{defn}{Identity map}{}
Given a set $X$, the \vocab{identity} $\mathrm{id}_X:X \to X$ is defined by $\mathrm{id}_X(x) = x$ for all $x \in X$.
\end{defn}

\begin{notation}
If the domain is unambiguous, the subscript may be removed.
\end{notation}
\pagebreak

\subsection{Injectivity, Surjectivity, Bijectivity}
\begin{defn}{Injectivity}{}
$f:X\to Y$ is \vocab{injective} if each element of $Y$ has at most one element of $X$ that maps to it.
\[ \forall x_1,x_2\in X,\:f(x_1)=f(x_2) \implies x_1=x_2 \]
\end{defn}

\begin{proposition}
If $f:X \to Y$ is injective and $g:Y \to Z$ is injective, then $g \circ f:X \to Z$ is injective.
\end{proposition}
\begin{proof}
Let $f:X \to Y$ and $g:Y \to Z$ be arbitrary injective functions. We want prove that the function $g \circ f:X \to Z$ is also injective.

To do so, we will prove $\forall x,x^\prime \in X$ that 
\[ (g \circ f)(x) = (g \circ f)(x^\prime) \implies x=x^\prime \]

Suppose that $(g \circ f)(x) = (g \circ f)(x^\prime)$. Expanding out the definition of $g \circ f$, this means that $g(f(x)) = g(f(x^\prime))$.

Since $g$ is injective and $g(f(x)) = g(f(x^\prime))$, we know $f(x)=f(x^\prime)$.

Similarly, since $f$ is injective and $f(x) = f(x^\prime)$, we know that $x=x^\prime$, as required.
\end{proof}

\begin{proposition}
$f$ is injective if and only if for any set $Z$ and any functions $g_1,g_2:Z\to X$ we have $f\circ g_1=f\circ g_2 \implies g_1=g_2$.
\end{proposition}
\begin{proof}
\textbf{Forward direction:}

If f is injective, we ultimately wish to show that $g_1=g_2$, so in order to do this we consider all possible inputs $z \in Z$, hoping to show that $g_1(z)=g_2(z)$.

But this is quite simple because we are given that $f\circ g_1=f\circ g_2$ and that $f$ is injective, so
\[ f \circ g_1(z)=f \circ g_2(z) \implies g_1(z)=g_2(z) \]

\textbf{Backward direction:}

We specifically pick $Z=\{1\}$, basically some random one-element set.

Then $\forall x,y \in X$, we define
\begin{align*}
& g_1:Z \to X, g_1(1)=x \\
& g_2:Z \to Y, g_2(1)=y \\
\end{align*}
Then
\[ f(x)=f(y) \implies f(g_1(1))=f(g_2(1)) \implies g_1(1)=g_2(1) \implies x=y \]
\end{proof}

\begin{defn}{Surjectivity}{}
$f:X\to Y$ is \vocab{surjective} if \emph{every} element of $Y$ is mapped to at least one element of $X$.
\[ \forall y\in Y,\:\exists x\in X \suchthat f(x)=y \]
\end{defn}

\begin{proposition}
If $f:X\to Y$ is surjective and $g:Y\to Z$ is surjective, then $g \circ f:X\to Z$ is surjective.
\end{proposition}
\begin{proof}
Let $f:X\to Y$ and $g:Y\to Z$ be arbitrary surjective functions. We want to prove that the function $g \circ f:X\to Z$ is subjective. 

To do so, we want to prove that for any $z \in Z$, there is some $x \in X$ such that $(g \circ f)(x) = z$. Equivalently, we want to prove that for any $z \in Z$, there is some $x \in X$ such that $g(f(x)) = z$.

Consider any $z \in Z$. Since $g:Y\to Z$ is surjective, there is some $y \in Y$ such that $g(y) = z$. Similarly, since $f:X\to Y$ is surjective, there is some $x \in X$ such that $f(x) = y$. This means that there is some $x \in X$ such that $g(f(x)) = g(y) = z$, as required.
\end{proof}

\begin{proposition}
$f$ is surjective if and only if for any set $Z$ and any functions $g_1,g_2:Y\to Z$ we have $g_1 \circ f=g_2 \circ f \implies g_1=g_2$.
\end{proposition}
\begin{proof}
\textbf{Forward direction:}

First we suppose that $f$ is surjective. Again, we wish to show that $g_1=g_2$, so we need to consider every possible input $y$ in Y. Then, since $f$ is injective, we can always pick $x \in X$ such that $f(x)=y$.

Then
\[ g_1 \circ f=g_2 \circ f \implies g_1 \circ f(x)=g_2 \circ f(x) \implies g_1(y)=g_2(y) \]

On the other hand, if $f$ is not surjective, then there exists $y \in Y$ such that for all $x \in X$ we have $f(x)\neq y$. We then aim to construct set $Z$ and $g_1,g_2:Y\to Z$ such that
\begin{enumerate}[label=(\roman*)]
\item $g_1(y) \neq g_2(y)$
\item $\forall y^\prime \neq y, g_1(y^\prime)=g_2(y^\prime)$
\end{enumerate}

Because if this is satisfied, then $\forall x \in X$, since $f(x)\neq y$ we have from (ii) that $g_1(f(x))=g_2(f(x))$; thus $g_1 \circ f=g_2 \circ f$, and yet from (i) we have $g_1 \neq g_2$.

\textbf{Backward direction:}

We construct $Z=Y\cup\{1,2\}$ for some random $1,2 \notin Y$.

Then we define
\begin{align*}
&g_1:Y\to Z,g_1(y)=1,g_1(y^\prime)=y^\prime
&g_2:Y\to Z,g_2(y)=2,g_2(y^\prime)=y^\prime
\end{align*}

Then when $y$ is not in the image of $f$, these two functions will satisfy $g_1 \circ f=g_2 \circ f$ but not $g_1=g_2$.

So conversely, if for any set $Z$ and any functions $g_i:Y \to Z$ we have $g_1 \circ f=g_2 \circ f \implies g_1=g_2$, such a value $y$ that is in the codomain but not in the range of $f$ cannot appear, and hence $f$ must be surjective.
\end{proof}

\begin{defn}{Bijectivity}{}
$f:X\to Y$ is \vocab{bijective} if it is both injective and surjective: each element of $Y$ is mapped to a unique element of $X$.
\end{defn}
\pagebreak

\subsection{Cardinality and countable sets}
When do two sets have the same size? Cantor answered this question in the 1800s, stating that two sets have the same size when you can pair each element in one set with a unique element in the other.

\begin{defn}{Cardinality}{}
$X$ and $Y$ have the same \vocab{cardinality} if there exists a bijection $f:X\to Y$, denoted by $|X| = |Y|$.
\end{defn}

\begin{defn}{Cardinality of finite sets}{}
The empty set $\emptyset$ is finite and has cardinality $|\emptyset| = 0$. A non-empty set $A$ is said to finite and have cardinality $|A| = n \in \NN$ if and only if there exists a bijection from $A$ to the set $\{1,2,\dots,n\}$.
\end{defn}

\begin{remark}
Note that for finite sets $X$ and $Y$, a function $f:X \to Y$ can only be \textbf{injective} if $|Y| \ge |X|$, since for any injective function the number of elements in the image $f(X)$, is equal to the number of elements in the domain, and $f(X) \subseteq Y$. In other words, the codomain of an injective function cannot be smaller than the domain.\footnote{This is sometimes referred to as the pigeonhole principle: if $n$ letters are placed in $m$ pigeonholes and $n > m$, then at least one hole must contain more than one letter; the non-injective function in that case is the assignment of pigeonholes to letters.}

Similarly, a function $f:X \to Y$ can only be \textbf{surjective} if $|Y| \le |X|$. Hence if $f$ is bijective, then $|X|=|Y|$; that is, the domain and codomain of a bijection have equal cardinality. (These results hold true for infinite sets too, though less obviously).
\end{remark}

\begin{thrm}{Cantor--Schr\"{o}der--Bernstein}{}
If $|X| \le |Y|$ and $|Y| \le |X|$ then $|X| = |Y|$.
\end{thrm}

\begin{defn}{Countably infinite set}{}
A set $X$ is \vocab{countably infinite} if it has the same cardinality as the set $\ZZ^{+}$ or $\NN$.
\end{defn}

\begin{defn}{Countable set}{}
A set $X$ is \vocab{countable} if it is either finite or infinitely countable.
\end{defn}

\begin{exmp}{}{}
$\{2n \mid n \in \NN\}$ is countably infinite, i.e. $|\{2n \mid n \in \NN\}| = |\NN|$. 

To prove this, define the function $f:\NN \to \{2n \mid n \in \NN\}$ as $f(n) = 2n$. Then, $f$ is injective -- if $f(n) = f(m)$ then $2n=2m \implies n=m$. Furthermore, $f$ is surjective, as if $m \in \{2n \mid n \in \NN\}$ then $\exists n \in \NN$ such that $m = 2n = f(n)$.
\end{exmp}

\begin{exmp}{}{}
$\ZZ^{+}$ is countable since we have a bijection $f:\ZZ^{+}\to\ZZ$ given by
\[ f(k)=\begin{cases}
    \dfrac{k}{2} & \text{ if } k \text{ is even } \\
    \dfrac{1-k}{2} & \text{ if } k \text{ is odd } \\
\end{cases} \]
In other words, $f(1)=0, f(2)=1, f(3)=-1, f(4)=2, f(5)=-2, f(6)=-3, \dots$ where our function $f$ stretches in both positive and negative directions.
\end{exmp}

\begin{thrm}{Cantor}{}
If $A$ is a set, then $|A|<|\mathcal{P}(A)|$.
\end{thrm}
\begin{proof}
Define the function $f:A \to \mathcal{P}(A)$ by $f(x) = \{x\}$. Then, $f$ is injective as $\{x\}=\{y\} \implies x=y$. Thus $|A| \le |\mathcal{P}(A)|$. To finish the proof now all we need to show is that $|A| \neq |\mathcal{P}(A)|$. We will do so through contradiction. Suppose that $|A| = |\mathcal{P}(A)|$. Then, there exists a surjection $g:A \to \mathcal{P}(A)$. We define the set $B$ as
\[ B \coloneq \{x \in A \mid x \notin g(x)\} \in \mathcal{P}(A) \]
Since $g$ is surjective, there exists a $b \in A$ such that $g(b) = B$. There are two cases:
\begin{enumerate}
\item $b \in B$. Then $b \notin g(b) = B \implies b \notin B$.
\item $b \notin B$. Then $b \notin g(b) = B \implies b \in B$.
\end{enumerate}
In either case we obtain a contradiction. Thus, $g$ is not surjective so $|A| \neq |\mathcal{P}(A)|$.
\end{proof}

\begin{corollary}
For all $n \in \NN \cup \{0\}$, $n<2^n$.
\end{corollary}
\begin{proof}
This can be easily proven through induction.
\end{proof}

\begin{lemma}
If $X$ is a countable set and $B \subseteq A$ then $B$ is countable.
\end{lemma}

\begin{lemma}
If $\{A_1,A_2,\dots\}$ is a collection of countably many countable sets then the set $\bigcup_{i=1}^\infty A_i$ is countable.
\end{lemma}

\begin{lemma}
If $\{A_1,A_2,\dots,A_n\}$ is a collection of finitely many countable sets then the set $A_1\times\cdots\times A_n$ is countable.
\end{lemma}
\pagebreak

\subsection{Composition of functions and invertibility}
\begin{defn}{Composition}{}
Given two functions $f:X\to Y$ and $g:Y\to Z$, the composition $g\circ f:X\to Z$ is defined by
\[ (g \circ f)(x) = g(f(x)) \quad \text{for all }x \in X. \]
\end{defn}

The composition of functions is not commutative, i.e. $f \circ g \neq g \circ f$. For example, if $f(x) = x^2$ and $g(x) = e^x$ are both maps from $\RR$ to $\RR$, then
\[ (f \circ g)(x) = e^{2x} \neq e^{x^2}= (g \circ f)(x) \]

However, composition is associative, as the following results shows:
\begin{proposition}[Associativity]
Let $f:X\to Y$, $g:Y\to Z$, $h:Z\to W$ be three functions. Then
\[ f \circ (g \circ h) = (f \circ g) \circ h. \]
\end{proposition}
\begin{proof}
Let $x \in X$. Then, by the definition of composition, we have
\[ (f \circ (g \circ h))(x) = f((g \circ h)(x)) = f(g(h(x))) = (f \circ g)(h(x)) = ((f \circ g) \circ h)(x). \]
\end{proof}

The following proposition addresses the extent to which composition of functions preserves injectivity and surjectivity:
\begin{proposition}
Let $f:X\to Y$ and $g:Y\to Z$ be functions.
\begin{enumerate}[label=(\roman*)]
\item If $f$ and $g$ are injective then so is $g \circ f$. Conversely, if $g \circ f$ is injective, then $f$ is injective, but g need not be.
\item If $f$ and $g$ are surjective then so is $g \circ f$. Conversely, if $g \circ f$ is surjective, then $g$ is surjective, but $f$ need not be.
\end{enumerate}
\end{proposition}
\begin{proof}
For the first part of (i), suppose $(g \circ f)(x_1) = (g \circ f)(x_2)$ for some $x_1, x_2 \in X$. From the injectivity of $g$ we know that $g(f(x_1)) = g(f(x_2))$ implies $f(x_1) = f(x_2)$, and then from the injectivity of $f$ we know that this implies $x_1 = x_2$. So $g \circ f$ is injective.

For the second part of (i), suppose $f(x_1) = f(x_2)$ for some $x_1, x_2 \in X$. Then applying g gives $g(f(x_1)) = g(f(x_2))$, and by the injectivity of $g \circ f$ this means $x_1 = x_2$. So $f$ is injective. To see that $g$ need not be injective, a counterexample is $X = Z = \{0\}, Y = \RR$, with $f(0) = 0$ and $g(y) = 0$ for all $y \in \RR$.
\end{proof}

Recalling that $\mathrm{id}_X$ is the identity map on $X$, we can define invertibility:
\begin{defn}{Invertibility}{}
A function $f:X\to Y$ is \vocab{invertible} if there exists a function $g:Y\to X$ such that $g \circ f = \mathrm{id}_X$ and $f \circ g = \mathrm{id}_Y$. The function $g$ is the inverse of $f$, denoted by $g = f^{-1}$.
\end{defn}

Note that directly from the definition, if $f$ is invertible then $f^{-1}$ is also invertible, and $(f^{-1})^{-1} = f$.

An immediate concern we might have is whether there could be multiple such functions $g$, in which case the inverse $f^{-1}$ would not be well-defined. This is resolved by the following result:

\begin{proposition}[Uniqueness of inverse]
If $f:X \to Y$ is invertible then its inverse is unique.
\end{proposition}
\begin{proof}
Let $g_1$ and $g_2$ be two functions for which $g_i \circ f = \mathrm{id}_X$ and $f \circ g_i = \mathrm{id}_Y$. Using the fact that composition is associative, and the definition of the identity maps, we can write
\[ g_1 = g_1 \circ \mathrm{id}_Y = g_1 \circ (f \circ g_2) = (g_1 \circ f) \circ g_2 = \mathrm{id}_X \circ g_2 = g_2 \]
\end{proof}

The following result shows how to invert the composition of invertible functions:

\begin{proposition}
Let $f:X \to Y$ and $g:Y \to Z$ be functions. If $f$ and $g$ are invertible, then $g \circ f$ is invertible, and $(g \circ f)^{-1} = f^{-1} \circ g^{-1}$.
\end{proposition}
\begin{proof}
Making repeated use of the fact that function composition is associative, and the definition of the inverses $f^{-1}$ and $g^{-1}$, we note that
\begin{align*}
(f^{-1}\circ g^{-1}) \circ (g \circ f) 
&= ((f^{-1} \circ g^{-1}) \circ g) \circ f \\
&= (f^{-1} \circ (g^{-1} \circ g)) \circ f \\
&= (f^{-1} \circ \mathrm{id}_Y) \circ f \\
&= f^{-1} \circ f \\
&= \mathrm{id}_X
\end{align*}
and similarly,
\begin{align*}
(g \circ f) \circ (f^{-1} \circ g^{-1}) 
&= g \circ (f \circ (f^{-1} \circ g^{-1})) \\
&= g \circ ((f \circ f^{-1}) \circ g^{-1}) \\
&= g \circ (\mathrm{id}_Y \circ g^{-1}) \\
&= g \circ g^{-1} \\
&= \mathrm{id}_Z
\end{align*}
which shows that $f^{-1} \circ g^{-1}$ satisfies the properties required to be the inverse of $g \circ f$.
\end{proof}

The following result provides an important and useful criterion for invertibility:

\begin{thrm}{}{}
A function $f:X \to Y$ is invertible if and only if it is bijective.
\end{thrm}
\begin{proof}
We prove this in both directions.

\textbf{Forward direction:}

Suppose $f$ is invertible, so it has an inverse $f^{-1}: Y \to X$. To show $f$ is injective, suppose that for some $x_1, x_2 \in X$ we have $f(x_1) = f(x_2)$. Then applying $f^{-1}$ to both sides and noting that by definition $f^{-1} \circ f = \mathrm{id}_X$, we see that $x_1 = f^{-1}(f(x_1)) = f^{-1}(f(x_2)) = x_2$. So $f$ is injective. To show that $f$ is surjective, let $y \in Y$, and note that $f^{-1}(y) \in X$ has the property that $f(f^{-1}(y)) = y$. So $f$ is surjective. Therefore $f$ is bijective.

\textbf{Backward direction:}

Suppose that $f$ is bijective, we aim to show that there is a well-defined $g:Y \to X$ such that $g \circ f = \mathrm{id}_X$ and $f \circ g = \mathrm{id}_Y$. Since $f$ is surjective, we know that for any $y \in Y$, there is an $x \in X$ such that $f(x) = y$. Furthermore, since $f$ is injective, we know that this $x$ is unique. So for each $y \in Y$ there is a unique $x \in X$ such that $f(x) = y$. This recipe provides a well-defined function $g(y) = x$, for which we have $g(f(x)) = x$ for any $x \in X$ and $f(g(y)) = y$ for any $y \in Y$. So $g$ satisfies the property required to be an inverse of $f$ and therefore $f$ is invertible.
\end{proof}

It is also possible to define left-inverse and right-inverse functions as functions that partially satisfy the definition of the inverse:

\begin{defn}{Left and right invertibility}{}
A function $f:X \to Y$ is \vocab{left invertible} if there exists a function $g:Y \to X$ such that $g \circ f = \mathrm{id}_X$, and is \vocab{right invertible} if there exists a function $h: Y \to X$ such that $f \circ h = \mathrm{id}_Y$.
\end{defn}

As may be somewhat apparent from the previous proof, being left- and right-invertible is equivalent to being injective and surjective, respectively. We leave this as an exercise to show.
\pagebreak

