\part{Real Analysis}
\chapter{Properties of the real numbers}
% https://math.libretexts.org/Bookshelves/Analysis/Introduction_to_Mathematical_Analysis_I_(Lafferriere_Lafferriere_and_Nguyen)
\section{Construction of the real numbers}
This book assumes familiarity with the rational numbers $\QQ$, i.e. numbers of the form $\dfrac{m}{n}$, where $m$, $n$ are integers and $n \neq 0$). 

$\QQ$ contains \emph{gaps} at irrational numbers such as $\sqrt{2}$ and $\pi$. In this section, we aim to construct $\RR$ from $\QQ$.

In 1872, German mathematician Richard Dedekind pointed out that a real number $x$ can be determined by its lower set $A$ and upper set $B$:
\[ A \coloneqq \{a:\QQ \mid a<x\} \]
\[ B \coloneqq \{b:\QQ \mid x<b\} \]
He defined a ``real number" as a pair of sets of rational numbers, the lower and upper sets shown above. Such a pair of sets of rational numbers are known as a \vocab{Dedekind cut}.

\begin{itemize}
\item $A$ is a \vocab{lower set}: $\forall a, b \in \RR$, if $a<b$ where $b \in A$, then $a \in A$.
\item $B$ is an \vocab{upper set}: $\forall a, b \in \RR$, if $a<b$ where $a \in B$, then $b \in B$.
\end{itemize}

\begin{defn}{Dedekind cut}{}
Given that $B$ is the complement of $A$ in the reals, a non-empty subset $(A, B) \subset \QQ$ is a Dedekind cut if:
\begin{enumerate}[label=\textbf{D\arabic*}]
\item $A$ is non-empty: $A \neq \emptyset$
\item $A$ and $B$ are disjoint: $A \cap B = \emptyset, A \cup B = \QQ$
\item $A$ is closed downwards: $\forall x,y \in \QQ$ with $x<y, y \in A$, then $x \in A$
\item $A$ does not contain a greatest element: $\forall x \in A, \exists y \in A$ such that $x<y$
\end{enumerate}
\end{defn}

Perhaps a not-so-intuitive fact here is that there are two possible things happening to $B$:
\begin{enumerate}
\item $B$ contains a least element
\item $B$ does not contain a least element
\end{enumerate}
Case 1 and 2 are known as rational and irrational Dedekind cuts respectively.

\begin{defn}{Real numbers}{} 
The set of real numbers $\RR$ is defined to be the set of all Dedekind cuts.
\end{defn}

\begin{remark}
The way we think about this is that Dedekind cuts are real numbers, and real numbers are Dedekind cuts.
\end{remark}
\pagebreak

\subsection{Order relations}
Given real numbers $\alpha$ and $\beta$, let $\alpha = (A,B)$ and $\beta = (C,D)$. Then
\[ \alpha < \beta \iff A \subset C \]
\begin{remark}
Since $B$ is the complement of $A$, $\alpha$ is completely determined by $A$ itself.
\end{remark}

This ordering on the real numbers satisfies the following properties:
\begin{itemize}
\item $x<y$ and $y<z$ $\implies$ $x<z$
\item Exactly one of $x<y$, $x=y$ or $x>y$ holds
\item $x<y \implies x+z<y+z$
\end{itemize}

\begin{property}[Ordering]
For any two real numbers $\alpha$ and $\beta$, one of the following must hold:
\[ \alpha < \beta \quad \alpha = \beta \quad \alpha > \beta \]
\end{property}

\begin{proof}
We prove by contradiction.

Note that $\alpha \le \beta \iff A \subseteq C$ ($A=C$ is possible).

Suppose otherwise, that all three of the above are false, then neither of the sets $A$ and $C$ can be a subset of the other.

We pick two rational numbers from each set:
Pick $p$ where $p \in A$, $p \notin C$, pick $q$ where $q \in C$, $q \notin A$
\begin{itemize}
\item Obviously we cannot have $p=q$.
\item If $p<q$, then since $q \in C$, according to property 3, we have $p \in C$, a contradiction.
\item Similarly for $p>q$, we would find that $q \in A$, a contradiction.
\end{itemize}

Hence our assumption is false.

$\therefore$ One of the three cases $\alpha < \beta$, $\alpha = \beta$, $\alpha > \beta$ must hold.
\end{proof}
\pagebreak

\subsection{Addition}
\begin{property}[Addition]
Let $\alpha = (A,B)$, $\beta = (C,D)$, then $\alpha + \beta = (X,Y)$ where
\[ X = \{a+c \mid a \in A, c \in C\} \]
\end{property}

\begin{proof}
To show that $(X,Y)$ is a Dedekind cut, we simply need to check the conditions for Dedekind cuts. 
\begin{itemize}
\item Property 1 is trivial.

\item Property 2 is by definition.

\item Property 3:

Let $x,y \in X$ satisfy $x<y$, $y \in X$. 

Let $y = a + c$, $a \in A$, $c \in C$.

Let $\epsilon = y - x$.

Let $a^\prime = a - \dfrac{\epsilon}{2}$, $c^\prime = c - \dfrac{\epsilon}{2}$.

Then \[ a^\prime + c^\prime = a + c - \epsilon = x \]
$a^\prime < a, a \in A \implies a^\prime \in A$. Similarly, $c^\prime \in C$.\\
$\therefore\:x = a^\prime +c^\prime \in X$.

\item Property 4:

$\forall a+c \in X, a \in A, c \in C$, $\exists a^\prime \in A, c^\prime \in C$ such that $a<a^\prime, c<c^\prime$.

$\therefore\:a^\prime +c^\prime \in X$ satisfies $a+c < a^\prime+c^\prime$.
\end{itemize}
\end{proof}

\begin{property}[Commutativity]
Addition is \textbf{commutative}:
\[ \alpha + \beta = \beta + \alpha \]
\end{property}

\begin{proof}
The proof is trivial.
\end{proof}

\begin{property}[Associativity]
Addition is \textbf{associative}:
\[ \alpha + (\beta + \gamma)=(\alpha + \beta)+ \gamma \]
\end{property}

\begin{proof}
Let $\alpha = (A,A^\prime)$, $\beta = (B,B^\prime)$, $\gamma = (C,C^\prime)$
\[ \beta + \gamma = (B+C, (B+C)^\prime) \]
In this notation we only need to show that $A+(B+C)=(A+B)+C$.
\begin{align*}
x \in A+(B+C) 
&\iff \exists a \in A, p \in B+C \suchthat x=a+p \\
&\iff \exists a \in A, b \in B, c \in C \suchthat x=a+b+c \\
&\iff x \in (A+B)+C
\end{align*}
Hence proven.
\end{proof}

\begin{exmp}{}{}
Prove that
\[ \alpha+0 = \alpha = 0+\alpha \]
\end{exmp}

\begin{proof}
Let $0=(O,O^\prime)$ where $O=\{x \mid x<0\}, O^\prime=\{x \mid x\ge0\}$.

Let $\alpha=(A,B)$, then $\alpha+0=(C,D)$ where
\begin{align*}
C&=\{a + \epsilon  \mid  a \in A, \epsilon<0\} \\
&=\{a - \epsilon  \mid  a \in A, \epsilon>0\}
\end{align*}
\[ a - \epsilon < a, a \in A \implies a - \epsilon \in A \implies C \subseteq A \]

According to Property 4, $\forall a \in A, \exists a^\prime \in A$ such that $a < a^\prime$.

Let $\epsilon = a^\prime - a > 0$, then 
\[ a = a^\prime - \epsilon, a^\prime \in A, \epsilon>0 \implies a \in C \]

So $A=C$.

$\therefore\:\alpha+0=\alpha$
\end{proof}

\begin{exmp}{}{}
Express $-\alpha$ in terms of $\alpha$; show
\[ \alpha+(-\alpha)=0=(-\alpha)+\alpha \]
\end{exmp}

\begin{proof}
We split this into two cases.

\textbf{Case 1}: $\alpha$ is a rational number, then $\alpha=(A,B)$ where $A = \{x \mid x < \alpha\}$, $B = \{x \mid x \ge \alpha\}$.

Let $-\alpha=(A^\prime,B^\prime)$, where $A^\prime = \{x \mid x < -\alpha\}$, $B^\prime = \{x \mid x\ge -\alpha\}$. 
We see that $\alpha+(-\alpha) \le 0$ is obvious.

On the other hand, since $0=(O,O^\prime)$, for any $\epsilon<0$ we have
\[ \epsilon = \brac{\alpha+\frac{\epsilon}{2}} + \brac{-\alpha+\frac{\epsilon}{2}} \in A+A^\prime \]
Hence $\alpha+(-\alpha)=0$.

\vspace{1cm}

\textbf{Case 2}: $\alpha$ is irrational, let $\alpha = (A,B)$ where $B$ does not have a lowest value. 
Then $-B = \{-x  \mid  x \in B\}$ does not have a highest value.

We wish to define $-\alpha=(-B,-A)$, but first we need to show that this is well-defined by checking through all the conditions.

\begin{itemize}
\item Property 1: This is trivial.

\item Property 2: Prove that $- A$ and $B$ are disjoint.

Note that $\forall x \in \RR$, if $x=-y$, then exactly one out of $y \in A$ and $y \in B$ is true $\implies$ exactly one out of $x \in -B$ and $x \in -A$ is true.

\item Property 3: Prove $-B$ is closed downwards.

Suppose otherwise, that $x<y, y \in -B$ but $x \notin -B$. Then $-y \in B$, $-x \notin B$. Since $A$ is the complement of $B$, $-y \notin A$, $-x \in A$. But $-y<-x$, which is a contradiction.

\item Property 4 is already guaranteed by the irrationality of $\alpha$.
\end{itemize}

All of these properties imply that the real numbers form a commutative group by addition.
\end{proof}

\subsection{Negation}
Given any set $X \subset \RR$, let $-X$ denote the set of the negatives of those rational numbers. That is $x \in X$ if and only if $-x \in -X$. 

If $(A,B)$ is a Dedekind cut, then $-(A,B)$ is defined to be
$(-B,-A)$.

This is pretty clearly a Dedekind cut. - proof

\subsection{Signs}
A Dedekind cut $(A,B)$ is \textbf{positive} if $0 \in A$ and \textbf{negative} if $0 \in B$. If $(A,B)$ is neither positive nor negative, then $(A,B)$ is the cut representing 0.

If $(A,B)$ is positive, then $-(A,B)$ is negative. Likewise, if $(A,B)$ is negative, then $-(A,B)$ is positive. The cut $(A,B)$ is non-negative if it is either positive or 0.

\subsection{Multiplication}
% Define multiplication of real numbers; you will need to define them for positive real numbers first

\subsubsection{Positive multiplication}
Let $\alpha = (A,B)$ and $\beta = (C,D)$ where $\alpha, \beta$ are both non-negative.

We define $\alpha \times \beta$ to be the pair $(X,Y)$ where

$X$ is the set of all products $ac$ where $a \in A, c \in C$ and at least one of the two numbers is non-negative. 
$Y$ is the set of all products $bd$ where $b \in B, d \in D$.

\subsubsection{General Multiplication}


% https://www.math.brown.edu/reschwar/INF/handout3.pdf

Intermediate Value Theorem

Bolzano-Weiersstrass Theorem

Connectedness of $\RR$


\section{Supremum and Infimum}
\subsection{Ordered sets}
Let $A$ be a set.

\begin{defn}{Order}{}
An \vocab{order} on $A$ is a relation, denoted by $<$, with the following two properties:
\begin{enumerate}[label=\textbf{O\arabic*}]
\item $\forall x,y \in A$, one and only one of the following statements is true:
\[ x<y, \quad x=y, \quad y<x \]
\item $\forall x,y,z \in A$, if $x<y$ and $y<z$, then $x<z$.
\end{enumerate}
\end{defn}

\begin{notation}
The notation $x \le y$ indicates that $x<y$ or $x = y$, without specifying which of these two is to hold. In other words, $x \le y$ is the negation of $x > y$.
\end{notation}

\begin{defn}{Ordered set}{}
An \vocab{ordered set} is a set $S$ in which an order is defined.
\end{defn}

For example, $\QQ$ is an ordered set if $r<s$ is defined to mean that $s-r$ is a positive rational number.

\subsection{Boundedness}
Let $A\subset\RR$.

\begin{defn}{Bounded}{}
$A$ is \vocab{bounded from above} if there exists an \vocab{upper bound} $M \in \RR$ such that $x \le M$ for all $x\in A$.

$A$ is \vocab{bounded from below} if there exists a \vocab{lower bound} $m \in \RR$ such that $x \ge m$ for all $x\in A$.

A is \vocab{bounded} in the real numbers if it is bounded above and below.
\end{defn}

\begin{defn}{Supremum}{}
The \vocab{supremum} of $A$, denoted by $\sup A$, is defined as the smallest real number $M$ such that $x \le M$ for all $x\in A$.
\begin{enumerate}[label=(\roman*)]
\item $M$ is an upper bound for $A$.
\item If $N$ is an upper bound for $A$, then $M \le N$.
\end{enumerate}
The suprenum is also known as the \emph{least upper bound}.
\end{defn}

\begin{remark}
If $M \in A$, then $M$ is the \textbf{maximum value} of $A$.
\end{remark}

The following proposition is convenient in working with suprema.
\begin{proposition}
Let $A$ be a nonempty subset of $\RR$ that is bounded above. Then  $M = \sup A$ if an only if
\begin{enumerate}[label=(\roman*)]
\item $x \le M$ for all $x \in A$
\item For any $\epsilon>0$, there exists $a \in A$ such that $M - \epsilon < a$.
\end{enumerate}
\end{proposition}
\begin{proof}
Suppose first that $M=\sup A$. Then clearly (i) holds (since this is identical to condition (1) in the definition of supremum). Now let $\epsilon>0$. Since $M-\epsilon<a$, condition (ii) in the definition of supremum implies that $M-\epsilon$ is not an upper bound of $A$. Therefore, there must exist an element $a$ in $A$ such that $M-\epsilon<a$, as desired.
\end{proof}

\begin{defn}{Infimum}{}
The \vocab{infimum} of $A$, denoted by $\inf A$, is defined as the largest real number $m$ such that $x \ge m$ for all $x\in A$.
\begin{enumerate}[label=(\roman*)]
\item $m$ is a lower bound for $A$.
\item If $n$ is a lower bound for $A$, then $m \ge n$.
\end{enumerate}
The infimum is also known as the \emph{greatest lower bound}.
\end{defn}

\begin{remark}
If $m \in A$, then $m$ is the \textbf{minimum value} of $A$.
\end{remark}

\begin{proposition}[Uniqueness of suprenum]
If a set $A \subset \RR$ has a supremum, then it is unique.
\end{proposition}

\begin{proof}
Assume that $M$ and $N$ are suprema of a set $A$.

Since $N$ is a supremum, it is an upper bound for $A$. Since $M$ is a supremum, then it is the least upper bound and thus $M \le N$. 

Similarly, since $M$ is a supremum, it is an upper bound for $A$; since $N$ is a supremum, it is a least upper bound and thus $N \le M$. 

Since $N \le M$ and $M \le N$, thus $M = N$. Therefore, a supremum for a set is unique if it exists.
\end{proof}

\begin{thrm}{Comparison Theorem}{}
Let $S, T \subset \RR$ be non-empty sets such that $s \le t$
for every $s \in S$ and $t \in T$. If $T$ has a supremum, then so does $S$, and $\sup S \le \sup T$.
\end{thrm}

\begin{proof}
Let $\tau = \sup T$. Since $\tau$ is a supremum for $T$, then $t \le \tau$ for all $t \in T$. Let $s \in S$ and choose any $t \in T$. Then, since $s \le t$ and $t \le \tau$ , then $s \le t$. Thus, $\tau$ is an upper bound for $S$. 

By the Completeness Axiom, $S$ has a supremum, say $\sigma = \sup S$. We will show that $\sigma \le \tau$. Notice that, by the above, $\tau$ is an upper bound for $S$. Since $\sigma$ is the least upper bound for $S$, then $\sigma \le \tau$. Therefore,
\[\sup S \le \sup T.\]
\end{proof}

Let's explore some useful properties of sup and inf.

\begin{proposition}
Let $S, T$ be non-empty subsets of $\RR$, with $S \subseteq T$ and with $T$ bounded above. Then $S$ is bounded above, and $\sup S \le \sup T$.
\end{proposition}
\begin{proof}
Since $T$ is bounded above, it has an upper bound, say $b$. Then $t \le b$ for all $t \in T$, so certainly $t \le b$ for all $t \in S$, so $b$ is an upper bound for $S$.

Now $S, T$ are non-empty and bounded above, so by completeness each has a supremum. Note that $\sup T$ is an upper bound for $T$ and hence also for $S$, so $\sup T \ge \sup S$ (since $\sup S$ is the least upper bound for $S$).
\end{proof}

\begin{proposition}
Let $T \subseteq \RR$ be non-empty and bounded below. Let $S = \{-t \mid t \in T\}$. Then $S$ is non-empty and bounded above. Furthermore, $\inf T$ exists, and $\inf T = -\sup S$.
\end{proposition}
\begin{proof}
Since $T$ is non-empty, so is $S$. Let $b$ be a lower bound for $T$, so $t \ge b$ for all $t \in T$. Then $-t \le -b$ for all $t \in T$, so $s \le -b$ for all $s \in S$, so $-b$ is an upper
bound for $S$.

Now $S$ is non-empty and bounded above, so by completeness it has a
supremum. Then $s \le \sup S$ for all $s \in S$, so $t \ge -\sup S$ for all $t \in T$, so $-\sup S$ is a lower bound for $T$.

Also, we saw before that if $b$ is a lower bound for $T$ then $-b$ is an upper bound for $S$. Then $-b \ge \sup S$ (since $\sup S$ is the least upper bound), so $b \le -\sup S$. So $-\sup S$ is the greatest lower bound.

So $\inf T$ exists and $\inf T = -\sup S$.
\end{proof}

\begin{proposition}[Approximation Property]
Let $S \subseteq \RR$ be non-empty and bounded above. For any $\epsilon > 0$, there is $s_\epsilon \in S$ such that $\sup S-\epsilon < s_\epsilon \le \sup S$.
\end{proposition}
\begin{proof}
Take $\epsilon > 0$.

Note that by definition of the supremum we have $s \le \sup S$ for all $s \in S$. Suppose, for a contradiction, that $\sup S-\epsilon \ge s$ for all $s \in S$.

Then $\sup S-\epsilon$ is an upper bound for $S$, but $\sup S-\epsilon < \sup S$, which is a contradiction.

Hence there is $s_\epsilon \in S$ with $\sup S-\epsilon<s_\epsilon$.
\end{proof}

\begin{thrm}{}{}
Any set bounded from above/below must have a supremum/infimum.
\end{thrm}
\begin{proof}
We prove the above theorem for supremum, using Dedekind cuts.

Let $S$ be a real number set. We consider the rational number set\footnote{very important that you remember this for the definition of Dedekind cuts}
\[ A = \{x \in \QQ \mid \exists y \in S, x<y\} \]

Now we go through the definitions to check that $(A|B)$ is a Dedekind cut
\begin{enumerate}
\item Since $S\neq\emptyset$, pick $y\in S$, then $[y]-1$ is a real number smaller than some element in $S$, hence $[y]-1 \in A$ and thus $A\neq\emptyset$.

Also, since we are given that $S$ is bounded, there exists a positive integer $M$ as an upper bound for $S$, thus $B\neq\emptyset$. (Note that an upper bound is simply a number that is bigger than anything from the set, and is not the supremum.)

\item We define $B$ to be the complement of $A$ in $\QQ$ so condition 2 is trivial.

\item For any $x,y\in A$, if $x<y$ and $y\in A$, then there exists $z\in S$ such that $y<z$, hence $x<z$ and thus $x\in A$.

\item Suppose otherwise that $x\in A$ is the largest element in $A$, then there exists $y\in S$ such that $x<y$.

We then pick a rational number $z$ between $x$ and $y$. Since we still have $z<y$, we have $z\in A$ but $z>x$, contradictory to $z$ being the largest.
\end{enumerate}

Now there is actually an issue with the proof for property 4 here: How exactly are we finding $z$?

First $x\in Q$. Then $y\in\RR$ so we rewrite it as $y=(C|D)$ via definition.

$x<y$ translates to the fact that $x\in C$. Now, since $y$ is real, by definition we know that $C$ must not have a largest element. In particular, $x$ is not largest and we can pick $z\in C$ such that $z>x$. This is in fact the $z$ that we need.

Now that all the properties of a real number are validated, we may finally conclude that $\alpha=(A|B)$ is indeed a real number.


Now we need to show that $\alpha$ is in fact the supremum of $S$.

Let $x\in S$. If $x$ is not the maximum value of $S$, i.e. $\exists y \in S$ such that $x<y$. Then $x\in A$ and thus $x<\alpha$.

If $x$ is the maximum value of $S$, then for any rational number $y<x$ we have $y\in A$, and for any rational number $y\ge x$ we have $y\in B$. Thus $x=(A|B)=\alpha$.

In conclusion, $x\le\alpha$ for all $x\in S$.

For any upper bound x of S, since $\forall y\in S, x\ge y$ we have $x\in B$ and thus $x\ge \alpha$.

Therefore, $\alpha$ is the smallest upper bound of $S$ and thus $\sup S=\alpha$ exists.
\end{proof}

\begin{prbm}
Consider the set $\{\frac{1}{n} \mid n\in\ZZ^{+}\}$.
\begin{enumerate}[label=(\alph*)]
\item Show that $\max S = 1$.
\item Show that if $d$ is a lower bound for $S$, then $d \le 0$.
\item Use (b) to show that $0 = \inf S$.
\end{enumerate}
\end{prbm}

\begin{proof}

\end{proof}

If we are dealing with rational numbers, the sup/inf of a set may not exist. For example, a set of numbers in $\QQ$, defined by $\{[\pi\cdot10^n]/10^n\}$.
3,3.1,3.14,3.141,3.1415,3.14159,...
But this set does not have an infimum in $\QQ$.

By ZFC, we have the Completeness Axiom, which states that any non-empty set $A \subset \RR$ that is bounded above has a supremum; in other words, if $A$ is a non-empty set of real numbers that is bounded above, there exists a $M \in \RR$ such that $M = \sup A$.




\begin{prbm}
Find, with proof, the supremum and/or infimum of $\{\frac{1}{n}\}$.
\end{prbm}

\begin{proof}
For the suprenum,
\[ \sup\crbrac{\frac{1}{n}} = \max\crbrac{\frac{1}{n}} = 1. \]
For the infinum, for all positive $a$ we can pick $n=[\frac{1}{a}]+1$, then $a>\frac{1}{n}$. Hence 
\[ \inf\crbrac{\frac{1}{n}}=0. \]
\end{proof}

\begin{prbm}
Find, with proof, the supremum and/or infimum of $\{\sin n\}$.
\end{prbm}

\begin{proof}
The answer is easy to guess: $\pm1$

For the supremum, we need to show that $1$ is the smallest we can pick, so for any $a=1-\epsilon<1$ we want to find an integer $n$ close enough to $2k\pi+\dfrac{\pi}{2}$ so that $\sin n > a$.

Whenever we want to show the approximations between rational and irrational numbers we should think of the \textbf{pigeonhole principle}.
\[ 2k\pi+\frac{\pi}{2}=6k+(2\pi-6)k+\frac{\pi}{2} \]
Consider the set of fractional parts $\{(2\pi-6)k\}$. Since this an infinite set, for any small number $\delta$ there is always two elements $\{(2\pi-6)a\}<\{(2\pi-6)b\}$ such that
\[ |\{(2\pi-6)b\}-\{(2\pi-6)a\}|<\epsilon \]

Then $\{(2\pi-6)(b-a)\}<\delta$

We then multiply by some number $m$ (basically adding one by one) so that
\[ 0 \le \{(2\pi-6)\cdot m(b-a)\}-\brac{2-\frac{\pi}{2}}<\delta \]

Picking $k=m(b-a)$ thus gives
\begin{align*}
2k\pi+\frac{\pi}{2} &= 6k+(2\pi-6)k+\frac{\pi}{2} \\
&= 6k+[(2\pi-6)k]+2+{(2\pi-6)k}-\brac{2-\frac{\pi}{2}}
\end{align*}

Thus $n=6k+[(2\pi-6)k]+2$ satisfies $\absolute{2k\pi+\dfrac{\pi}{2}-n}<\delta$

Now we're not exactly done here because we still need to talk about how well $\sin n$ approximates to 1.

We need one trigonometric fact: $\sin x<x$ for $x>0$. (This simply states that the area of a sector in the unit circle is larger than the triangle determined by its endpoints.)

\begin{align*}
\sin n &= sin\brac{n-\brac{2k\pi+\frac{\pi}{2}}+\brac{2k\pi+\frac{\pi}{2}}} \\
&= \cos\brac{n-\brac{2k\pi+\frac{\pi}{2}}} \\
&= \cos\theta
\end{align*}

\[ 1 - \sin n = 2 \sin^2 \frac{\theta}{2} = 2 \sin^2 \absolute{\frac{\theta}{2}} \le \frac{\theta^2}{2}<\delta \]

Hence we simply pick $\delta=\epsilon$ to ensure that $1 - \sin n<\epsilon$, and we're done.
\end{proof}

\begin{thrm}{Archimedean Principle}{}
If $a,b \in \RR$ with $a>0$, then there exists $n \in \NN$ such that $na>b$.
\end{thrm}
\begin{proof}
Suppose that the Archimedean Property is false. Then there exists $a, b \in \RR, a > 0$ such that $na \le b$ for all $n \in \NN$.

For these particular $a$ and $b$, we can say that $b$ is an upper bound of $S \coloneqq \{na \mid n \in \NN\}$. From the completeness axiom, $s_0 \coloneqq \sup S$ exists. Let $n \in \NN$, we have $n+1 \in \NN$. So $s_0 \ge (n+1)a = na+a$.

Then we have $s_0-a \ge na$. This is true for all $n \in \NN$. So $s_0-a$ is an upper bound of $S$. However, $s_0-a<s_0$, which contradicts that $s_0$ is the least upper bound of $S$. This contradiction shows that the Archimedean Property is true.
\end{proof}
% https://mth32015.files.wordpress.com/2015/01/jan-26-30.pdf
\pagebreak

\section{Completeness}
\subsection{Completeness axiom}
\begin{thrm}{Completeness axiom for the real numbers}{}
Let $A$ be a non-empty subset of $\RR$ that is bounded above. Then $A$ has a supremum.
\end{thrm}

Any set in the reals bounded from above/below must have a supremum/infimum.

\begin{proof}
We prove this using Dedekind cuts.

Let $S$ be a real number set. 
We consider the rational number set $A = \{x \in \QQ \mid \exists y \in S\}$. Set $B$ is defined to be the complement of $A$ in $\QQ$.

We go through the definitions to check that $(A|B)$ is a Dedekind cut.
\begin{enumerate}
\item Since $S \neq \emptyset$, pick $y \in S$, then $[y]-1$ is a real number smaller than some element in $S$, hence $[y]-1 \in A$ and thus $A \neq \emptyset$.

Since we're given that $S$ is bounded, $\exists M>0$ as the upper bound for $S$, thus $B \neq \emptyset$.

(Note that an upper bound is simply a number that is bigger than anything from the set, and is not the supremum

\item We defined $B$ to be the complement of $A$ in $\QQ$, so this condition is trivial.

\item For any $x,y \in A$, if $x<y$ and $y\in A$, then $\exists z \in S$ such that $y<z \implies x<z \implies x \in A$.

\item Suppose otherwise that $x \in A$ is the largest element in A, then $\exists y \in S$ such that $x<y$
We then pick a rational number $z$ between $x$ and $y$. 
Since we still have $z<y$, we have $z \in A$ but $z>x$, contradictory to $z$ being the largest.

Now there's actually an issue with the proof for property 4 here
How exactly are we finding z?

First $x \in \QQ$. 
Then $y \in \RR$ so we rewrite it as $y=(C|D)$ via definition.

$x<y$ translates to the fact that $x \in C$.

Since $y$ is real, by definition we know that $C$ must not have a largest element.

In particular, $x$ is not largest and we can pick $z \in C$ such that $z>x$. 
This is in fact the $z$ that we need
\end{enumerate}

Now that all the properties of a real number are validated, we may finally conclude that $\alpha=(A|B)$ is indeed a real number.

Now we need to show that $\alpha = \sup S$.

Let $x \in S$. 
If $x$ is not the maximum value of $S$, i.e. $\exists y \in S,x<y$, then $x \in A$ and thus $x<\alpha$.

If $x$ is the maximum value of $S$, then for any rational number $y<x$ we have $y \in A$, and for any rational number $y \ge x$ we have $y \in B$.
Thus $x=(A|B)=\alpha$.

In conclusion, $x \le \alpha$ for all $x \in S$.

For any upper bound $x$ of $S$, since $\forall y \in S, x \ge y$ we have $x \in B$ and thus $x \ge \alpha$.

$\therefore$ $\alpha$ is the smallest upper bound of $S$ and thus $\sup S = \alpha$ exists.
\end{proof}

\begin{thrm}{Archimedean property of $\NN$}{}
$\NN$ is not bounded above.
\end{thrm}
\begin{proof}
Suppose, for a contradiction, that $\NN$ is bounded above. Then $\NN$ is non-empty and bounded above, so by completeness (of $\RR$) $\NN$ has a supremum.

By the Approximation property with $\epsilon=\frac{1}{2}$, there is a natural number $n\in\NN$ such that $\sup\NN-\frac{1}{2}<n\le\sup\NN$.

Now $n+1\in\NN$ and $n+1>\sup\NN$. This is a contradiction.
\end{proof}

\section{Order properties of the real numbers}

\section{Topological properties of the real numbers}

\chapter{Sequences and Series}
\textbf{References:} Rudin (Chapter 3)

\section{Limit of a sequence}
\subsection{Definition}
\begin{defn}{Convergence of sequence}{}
Let $\{a_n\}$ be a real sequence, let $L \in \RR$. We say that $\{a_n\}$ \textbf{converges} to $L$ as $n \to \infty$ if
\[ \forall\epsilon>0,\:\exists N \in\NN \text{ such that } \forall n \ge N,\:|a_n-L| < \epsilon. \]
In this case we write $a_n \to L$ as $n \to \infty$, and we say that $L$ is the \textbf{limit} of $\{a_n\}$.

If $\{a_n\}$ does not converge, then we say that it \textbf{diverges}.
\end{defn}

\begin{remark}
Take note of the use of logical statements:
\begin{itemize}
\item $\epsilon$ is independent, so it is literally for all $\epsilon>0$.
\item $N$ is dependent on $\epsilon$; if $\epsilon$ is very small we would expect the sequence $\{a_n\}$ to get close enough to $L$ further down the line.
\item The order of the quantifiers matters.
\end{itemize}
\end{remark}

\begin{exmp}{}{}
What do we really mean by saying that $\frac{1}{n}\to 0$ as $n\to\infty$?

We mean that the sequence of numbers $\frac{1}{n}$ converges to 0, proven as follows:

\begin{proof}
$\forall\epsilon>0$, pick $N=\frac{1}{\epsilon}+1$. Then $\forall n>N$,
\[ \frac{1}{n} < \frac{1}{N} < \frac{1}{\frac{1}{\epsilon}} = \epsilon. \]
\end{proof}
\end{exmp}

\subsection{Characteristics of limits}
\begin{enumerate}
\item Given a sequence of points $\{x_k\}$ and a point $x\in\RR^n$, $x_k$ converges to $x$ if and only if all neighbourhoods of x ``eventually" contain all $x_k$.

By eventually we mean something similar to the definition above: there exists some large $N$ such that the property is satisfied for all $n>N$.

\begin{proof}

\textbf{Forward direction:} 

If $\{x_k\}$ converges to $x$, we wish to prove: given any neighbourhood $U$ of $x$, $U$ eventually contains all $x_k$.

Since $U$ is a neighbourhood of $x$, we pick a ball of radius $\epsilon$ centered at x, $B(x,\epsilon)$, so that $B(x,\epsilon)$ is contained in $U$.

Then since $B(x,\epsilon)$ is precisely the set of points whose distance to $x$ is no larger than $\epsilon$, we then apply the fact that $\{x_k\}$ converges to $x$.

So for this particular $\epsilon$, we take a natural number $N$ so that $|x_k-x|<\epsilon$, or $x_k \in B(x,\epsilon)$, for all $k>N$.

Then simultaneously $x_k$ are in $U$ since $B(x,\epsilon)$ is a subset of $U$, thus we've shown that $U$ will contain all $x_k$ after a certain point $N$.

\vspace{.5cm}

\textbf{Backward direction:}

Suppose that all neighbourhoods of $x$ will eventually contain all $x_k$, then in particular for any $\epsilon>0$, since $B(x,\epsilon)$ is a neighbourhood of $x$, it will also eventually contain all $x_k$.

This then easily translates to the fact that $\{x_k\}$ converges to $x$.
\end{proof}

\item \textbf{Uniqueness of the limit}

Suppose that $\{x_k\}$ converges to both $x$ and $x^\prime$, then $x=x^\prime$.

\begin{proof}
$\forall \epsilon>0$, we know that the terms in $\{x_k\}$ must be less than $\epsilon$ away from its limit after a certain point. 

However, this certain point may not be the same for both limits; for the two limits $x$ and $x^\prime$, we must first assume two separate numbers $N$ and $N^\prime$ so that $|x_k-x|<\epsilon$ when $k>N$, and $|x_k-x^\prime|<\epsilon$ when $k>N^\prime$.

Now if you look at the book here, it says that we have a stronger requirement:
$|x_k-x|<\epsilon/2$ when $k>N$,
$|x_k-x^\prime|<\epsilon/2$ when $k>N^\prime$.
This is simply because we want to prove certain statements strictly by definition



There is an important detail to take note, regarding $\max\{N,N^\prime\}$.

We're taking the larger one of these, so it means that, after this certain point, we in fact have $|x_k-x|<\frac{\epsilon}{2}$ and $|x_k-x^\prime|<\frac{\epsilon}{2}$ at the same time.

Therefore by triangle inequality,
\[ |x-x^\prime| \le |x_k-x| + |x_k-x|<\epsilon \]
The choice of k actually vanished in the final statement; you can think of this as if picking this particular choice of k helps us to establish some kind of property for the original objects

Finally, since we've in fact proven that $|x-x^\prime|<\epsilon$ holds for any given positive $\epsilon>0$, we must have $|x-x^\prime|=0$ and therefore $x=x^\prime$.

Strictly speaking, for the first part we need to explain why $a<\epsilon$ for any positive $\epsilon$ implies that $a \le 0$. 
This is very easy to prove (by contradiction) so let's not be too redundant
The second part simply relies on the fact that |x-y| is the Euclidean metric and so by positive definiteness |x-y|=0 if and only if x=y.
\end{proof}

\item \textbf{Boundedness of converging sequences}

If $\{x_k\}$ converges, then $\{x_k\}$ is bounded.

Obviously this doesn't work the other way around

We simply take the limit $x$ and note that the sequence is eventually contained in some ball centered at $x$, say $B(x,1)$.

There are several outlying points prior to this, but since there are only a finite number of these, it doesn't change the fact that the sequence (viewed as a set) is bounded nevertheless.

This argument is precisely expressed by the construction of r given in the book: let $|x_k-x|<1$ whenever $k>N$, then $\{x_k\}$ is in $B(x,r)$ where $r=\max\{1,|x_1-x|,\dots,|x_N-x|\}$

\item We talk about the relationship between the limit of a sequence and the limit points of a set.

Generally, limit points are a weaker construction.


Suppose that $\{x_k\}$ converges to $x$
If we view $\{x_k\}$ as a set, then $x$ will be a limit point of this set

The converse, however, is not true

Exercise 1: Construct a sequence in R that is bounded and contains a single limit point but is divergent (not convergent)

The thing about convergence of a series is that, unlike for limit points where we only require that there are other points that get arbitrarily close, but moreover we have to ensure that this pattern ensues for each and every term in the sequence

Me:Suppose that $\{x_k\}$ converges to $x$
If we view $\{x_k\}$ as a set, then $x$ will be a limit point of this set”
- - - - - - - - - - - - - - -
Sorry I forgot something crucial about this
:
There is the strange possibility that the sequence $\{x_k\}$ is constant
:
(or at least eventually constant)
:
Then in fact $x$ by definition is not a limit point of $x_k$ because you can find a ball around $x$ that only contains the element $x$ itself, since that point is merely what the entire sequence $\{x_k\}$ amounts to
:
Anyways, we simply can't say that a sequence $\{x_k\}$ converges to $x$ if we're only provided with the fact that $x$ is a limit point of $\{x_k\}$

However, we can say the following:
(d) If $x$ is a limit point of $E$, then there exists a sequence $\{x_n\}$ in $E\setminus x$ such that $\{x_n\}$ converges to $x$

In fact this is correct in both ways so let's rewrite this as follows:
(d) x is a limit point of $E$, if and only if there exists a sequence $\{x_n\}$ in $E\setminus x$ such that $\{x_n\}$ converges to $x$

($E\setminus x$ is important here, otherwise we simply pick the constant sequence $x_k=x$)

→: If x is a limit point, then for all $\epsilon>0$, $B_0(x,\epsilon)$ contains points in $E$
We then construct such a sequence $\{x_k\}$ in $E\setminus x$: pick any $x_k \in E$ so that $x_k$ is contained in $B_0(x,1/k)$

Then it is easy to show that $\{x_k\}$ is a sequence in $E\setminus x$ which converges to $x$.

←: Suppose that there exists a sequence $\{x_n\}$ in $E\setminus x$ such that $\{x_n\}$ converges to $x$
We wish to show that $B_0(x,\epsilon)$ contains points in $E$ for all $\epsilon>0$

Since $\{x_n\}$ converges to $x$, for all $\epsilon>0$ the sequence is eventually contained in $B(x,\epsilon)$
However because we have the precondition that $\{x_n\}$ has to be in $E\setminus x$, the sequence is in fact eventually contained in $B_0(x,\epsilon)$.
\end{enumerate}

\section{Subsequences}


Properties:
\begin{enumerate}
\item $\{x_k\}$ converges to $x$ if and only if every subsequence of $\{x_k\}$ converges to $x$.

We only need to prove this in the forwards direction
Every subsequence of $\{x_k\}$ can be written in the form $\{x_{k_i}\}$ where $k_1<k_2<\dots$ is a strictly increasing sequence of natural numbers

Intuitively, if every neighbourhood of x eventually contains all $x_k$, then since $\{x_{k_i}\}$ is just a subset of $\{x_k\}$ they should all be contained in the neighbourhood eventually as well.
For every $\epsilon>0$, pick $N$ such that for $k>N$, $|x_k-x|<\epsilon$.
Pick $M$ such that $k_M>N$, then for all $i>M$ we have $|x_(k_i)-x|<\epsilon$.

\item Subsequential limits of a sequence are precisely the limit points of the sequence (viewed as a set)

This is just part (d) of the previous section.

Again, to make this work, we need to assume that nothing funny is going on at subsequential limits
If the limits appear due to eventually constant subsequences, then they need not be limit points of the original sequence when viewed as a set

3.6, 3.7 are precisely the statements we've prepared for last week

\item If $\{x_n\}$ is a sequence in a compact set (bounded closed set), then there exists a convergent subsequence of $\{x_n\}$
This is Weierstrass-Bolzano together with part (b)

Ah yes, regarding compact sets
I need to emphasize this again, but the definition that we are currently using for compact sets is not the actual definition

I've sent a video before the lesson which talks about the real definition for compact sets
Essentially, compact sets satisfies the property akin to the statement in Heine-Borel:
Given a topological space $(X,\tau)$, a compact set $K$ in $X$ is a set satisfying that, given any open covering $\{U_i\}$ of $X$, there exists a finite open cover $\{U_1,\dots,U_n\}$ of $X$

This is difficult to process at this stage
Since we're currently only working with Euclidean spaces it would be more beneficial if you consider the Heine-Borel Theorem as a property first
It would be a lot easier to accept the definition after you're more accustomed to applying the theorem

\item (Rudin 3.7) Subsequential limits form a closed subset

Actually we've done this two weeks before, it is simply saying that A'' is a subset of A'.


(A'' is not always A'; consider the set in R² given by
{(1/n,1/m)|n,m in N}
Then (1,0),(0,1) are in A' but not in A''
\end{enumerate}

\section{Cauchy Sequences}
\begin{defn}{Cauchy sequence}{}
A sequence $\{x_k\}$ in $\RR^n$ is a \textbf{Cauchy sequence}, if the distances between any two terms is sufficiently small after a certain point.

Formally, this is given by: $\forall \epsilon>0$, there exists integer $N$ such that 
\[ \forall k,l>N, |x_k-x_l|<\epsilon. \]
\end{defn}

It is easy to prove that a converging sequence is Cauchy using the triangle inequality. The idea is that, if all the points are becoming arbitrarily close to a given point p, then they are also becoming close to each other. The converse is not always true, however.

\begin{lemma}
A sequence $\{x_k\}$ in $\RR^n$ is convergent if and only if it is Cauchy.
\end{lemma}

\begin{proof}
\textbf{Forward direction:}

Suppose that $\{x_k\}$ converges to $x$, then there exists $N$ such that for $k>N$, $|x_k-x|<\dfrac{\epsilon}{2}$
Then for $k,l>N$, 
\[ |x-k-x_l| \le |x_k-x|+|x_l-x| < \epsilon \]

\textbf{Backward direction:}

First, we show that $\{x_k\}$ must be bounded. 
Pick $N$ such that for all $k,l>N$ we have $|x_k-x_l|<1$. 
Centered at $x_k$, we show that $\{x_k\}$ is bounded; to do this we pick
\[ r = \max\{1,|x_k-x_1|,\dots,|x_k-x_N|\} \]
Then the sequence ${x_k}$ is in $B(x_k,r)$ and thus is bounded.

Since $\{x_k\}$ is bounded, by the collolary of Bolzano-Weierstrass we know that $\{x_k\}$ contains a subsequence $\{x_{k_i}\}$ that converges to a limit $x$.

Then for all $\epsilon>0$, pick $N_1$ such that for all $k,l>N$, $|x_k-x_l|<\dfrac{\epsilon}{2}$. 
Simultaneously, since $\{x_{k_i}\}$ converges to $x$, pick $M$ such that for $i>M$, $|x_{k_i}-x|<\dfrac{\epsilon}{2}$.

Now, since $k_1<k_2<\dots$ is a sequence of strictly increasing natural numbers, we can pick $i>M$ such that $k_i>N$. Then for all $k>N$, by setting $l=k_i$ we obtain
\[ |x_k-x_{k_i}| < \frac{\epsilon}{2}, \quad |x_{k_i}-x| < \frac{\epsilon}{2} \]
and hence
\[ |x_k-x| \le |x_k-x_{k_i}|+|x_{k_i}-x| < \epsilon \]
\end{proof}

\section{Upper and Lower Limits}

\section{Limits of multiple sequences}

\chapter{Continuity}
\section{Limit of Functions}
\begin{defn}{Limit}{}
Let $X$ and $Y$ be metric spaces; suppose $E \subset X$, $f:E\to Y$ and $p$ is a limit point of $E$. We write $f(x) \to q$ as $x \to p$, or
\[ \lim_{x\to p}f(x)=q \]
if there is a point $q \in Y$ with the following property: $\forall \epsilon > 0 \exists \delta > 0$ such that \[ d_Y(f(x), q) < \epsilon \] for all points $x \in E$ for which \[ 0 < d_X(x, p) < \delta. \]
\end{defn}

\begin{remark}
The symbols $d_X$ and $d_Y$ refer to the distances in $X$ and $Y$ respectively. If $X$ and/or $Y$ are replaced by the real line, the complex plane, or some euclidean space $\RR^k$, the distances $d_X$ and $d_Y$ are replaced by absolute values, or by norms of differences.
\end{remark}

We can recast this definition in terms of limits of sequences:
\[ \lim_{n\to\infty}f(p_n)=q \]
for every sequence $(p_n) \in E$ so that $p_n \neq p$ and $\lim_{n\to\infty} p_n = p$.

By the same proofs as for sequences, limits are unique, and in R they add/multiply/divide as expected.

\begin{defn}{Continuity}{}
$f$ is continuous at $p$ if
\[ \lim_{x\to p}f(x) = f(p). \]
In the case where $p$ is not a limit point of the domain $E$, we say $f$ is continuous at $p$. If $f$ is continuous at all points of $E$, then we say $f$ is continuous on $E$.
\end{defn}

\section{Continuous Functions}

\section{Continuity and Compactness}

\section{Continuity and Connectedness}

\section{Discontinuities}

\section{Monotonic Functions}

\section{Infinite Limits and Limits at Infinity}

\chapter{Sequences and Series of Functions}

\chapter{Differentiation}
In this section we will be discussing about differentiation of a real function.

You might have already heard of the definition of the derivative:
\[ f^\prime(c)=\lim_{x\to c}\frac{f(x)-f(c)}{x-c} \]

$f$ is \textbf{differentiable} at $x$ if $f^\prime(x)$ exists, and we say that $f$ is differentiable on a set $E$ if $f$ is differentiable at every $x \in E$.

One particular instance to take note here is if $E$ is an open interval $(a,b)$

Later on when we talk about properties of differentiation such as the intermediate value theorems, we usually have the following requirement on the function:
$f$ is a continuous function on $[a,b]$ which is differentiable in $(a,b)$.

This is a slightly weaker condition than '$f$ is differentiable on $[a,b]$', for example $f(x)=\sqrt{1-x^2}$ satisfies this property on $[-1,1]$ and therefore the properties of differentiation still apply

\begin{exercise}{}{}
Prove the product and quotient rules of differentiation.
\end{exercise}

\begin{exercise}{}{}
Prove the chain rule.
\end{exercise}
\begin{proof}
Now the proof for chain rule is actually quite nontrivial
The proof of the chain rule that is usually taught in high school actually has some holes in it

First we list out the conditions:
1. $f$ is continuous on some interval $[a,b]$ and differentiable at $x\in[a,b]$ (if $x$ is either $a$ or $b$, then the limit taken in the derivative is a right or left limit).
2. $g$ is a function defined on the range of $f$ (otherwise $g\circ f$ wouldn't exist) and differentiable at $f(x)$.

Again the notion of limits here must be taken over all 'relevant points'

For example let $g(x)=x$ be defined only on the non-negative real numbers, and let $f(x)=x^2$.

Then $g\circ f(x)=x^2$ is defined, and in fact differentiable over $\RR$, even though there is a worrying issue at $x=0$ (here $f(0)=0$ so we will be dealing with an extremal input for $g$)

The saving grace here is that $g$ is still differentiable at $0$, even though here we need to interpret differentiability through a right limit: $\lim_{x\to0^+}\frac{g(x)}{x}=1$, but $\lim_{x\to0^-}\frac{g(x)}{x}$ does not exist.

If $g$ does not even have such differentiability at the endpoint, then we cannot expect $g\circ f$ to be differentiable, e.g. $g(x)=\sqrt{x}$ and $f(x)=x^2$, then $g(f(x))=|x|$ is not differentiable at $x=0$.

Now let's dive into the proof

We know that
\[ f^\prime(x)=\lim_{t\to x}\frac{f(t)-f(x)}{t-x}, \]
so under the assumption that $t$ stays within the domain of $f$, $\frac{f(t)-f(x)}{t-x}$ should be a good approximation to $f^\prime(x)$.

To actually quantify this, let $u(t)=\frac{f(t)-f(x)}{t-x}-f^\prime(x)$.

Then the differentiability of $f$ tells us that $\lim_{t\to x}u(t)=0$.

Similarly, let $v(s)=\frac{g(s)-g(y)}{s-y}-g^\prime(y)$, then $\lim_{s\to y}v(s)=0$, as long as $s$ stays in the domain of $g$
(in my opinion it is quite helpful to think of these limits as sequential ones)

What's nice here is that we can let $s=f(t)$, then by our assumption s always stays in the domain of g, so nothing fishy will happen

Ah I forgot a small detail here
Additionally we also need to define u(x)=0 and v(y)=0

Now let $h(t)=g(f(t))$, then $h$ is defined on $[a,b]$, and we deduce that
\[ h(t)-h(x)=(t-x)[f^\prime(x)+u(t)][g^\prime(y)+v(s)] \]

We then check that
\[ \lim_{t\to x}\frac{h(t)-h(x)}{t-x} = \lim_{t\to x}[f^\prime(x)+u(t)][g^\prime(y)+v(s)] = f^\prime(x)g^\prime(f(x)) \]
and we are done.
\end{proof}

\begin{exmp}{}{}
One of the best (worst?) family of pathological examples in calculus are functions of the form
\[ f(x)=x^p\sin\frac{1}{x}. \]

Here we're given the examples where p=1 and p=2
For p=1, the function is continuous and differentiable everywhere other than x=0
For p=2, the function is differentiable everywhere, but the derivative is discontinuous

Other more advanced pathological results (just for fun):
1. The graph for $y=\sin\frac{1}{x} 1/x$ on (0,1], together with the interval [-1,1] on the y-axis, is a connected closed set that is not path-connected

2. For $0<p<1$, we obtain functions that are continuous and bounded, but the graphs are of infinite length (ps. I think that this is also true for p=1)
\end{exmp}

\section{Mean Value Theorems}
We say that $x$ is a \vocab{stationary point} of $f$ if $f^\prime(x)=0$.

\vocab{Local maximum/minimum} are points which attains maximum/minimum in some neighbourhood of that particular point.

\begin{thrm}{Fermat's Theorem (Interior Extremum Theorem)}{}
If the differential exists, then by comparing the left and right limits it is easy to see that the differential for a local maximum/maximum can only be $0$.

To summarize in four words: Local extrema are stationary
\end{thrm}

There are three mean value theorems, from specific to general:
\begin{enumerate}
\item Rolle's Theorem
\item (Lagrange's) Mean Value Theorem
\item Generalised (Cauchy's) Mean Value Theorem
\end{enumerate}

\begin{thrm}{Rolle's Theorem}{}
If $f$ is continuous on $[a,b]$, differentiable in $(a,b)$ and $f(a)=f(b)$, then there exists \[ c\in(a,b) \suchthat f^\prime(c)=0. \]
\end{thrm}

\begin{proof}
Let $h(x)$ be a function defined on $[a,b]$ where $h(a)=h(b)$.

The idea is to show that $h$ has a local maximum/minimum, then by Fermat's Theorem this will then be the stationary point that we're trying to find.

First note that $h$ is continuous on $[a,b]$, so $h$ must have a maximum $M$ and a minimum $m$.

If $M$ and $m$ were both equal to $h(a)=h(b)$, then $h$ is just a constant function and so $h^\prime(x)=0$ everywhere.

Otherwise, $h$ has a maximum/minimum that is not $h(a)=h(b)$, so this extremal point lies in $(a,b)$.

In particular, this extremal point is also a local extremum.
Since $h$ is differentiable on $(a,b)$, by Fermat's theorem this extremum point is stationary, thus Rolle's Theorem is proven.
\end{proof}

\begin{defn}{Mean Value Theorem}{}
If $f$ is continuous on $[a,b]$ and differentiable in $(a,b)$, then there exists \[ c\in(a,b) \suchthat f^\prime(c)=\frac{f(b)-f(a)}{b-a}. \]
\end{defn}

Exercise 2: Show that the Mean Value Theorem results directly from Rolle's Theorem (the other direction is trivial)
:
This isn't a very significant exercise because we're going to prove something more general

\begin{thrm}{Generalised Mean Value Theorem}{}
If f and g are continuous on [a,b] and differentiable in (a,b), then there exists \[ c\in(a,b) \suchthat [g(b)-g(a)]f^\prime(c)=[f(b)-f(a)]g^\prime(c). \]
\end{thrm}


Now we return to the proof of the generalized MVT

We set the function $h(t)=[f(b)-f(a)]g(t)-[g(b)-g(a)]f(t)$, then $h$ is continuous on $[a,b]$ and differentiable on $(a,b)$

Moreover, $h(a)=f(b)g(a)-f(a)g(b)=h(b)$, thus by Rolle's Theorem, there exists $c\in(a,b)$ such that $h^\prime(c)=0$, i.e. $[g(b)-g(a)]f^\prime(c)=[f(b)-f(a)]g^\prime(c)$

Corollary: If $f$ and $g$ are continuous on $[a,b]$ and differentiable in $(a,b)$, and $g^\prime(x)\neq0$ for all $x\in(a,b)$, then there exists \[ c\in(a,b) \suchthat f^\prime(c)/g^\prime(c)=[f(b)-f(a)]/[g(b)-g(a)] \]

This form of the generalized MVT will be used to prove the most beloved rule of high school students

exercises for the Mean Value Theorem

\begin{exercise}{}{}
Let $f$ and $g$ be continuous on $[a,b]$ and differentiable on $(a,b)$. If $f^\prime(x)=g^\prime(x)$, then $f(x)=g(x)+C$.
\end{exercise}

\begin{exercise}{}{}
Given that $f(x)=x^\alpha$ where $0<\alpha<1$. Prove that $f$ is uniformly continuous on $[0,+\infty)$.
\end{exercise}

\begin{exercise}{Olympiad level}{}
Let $f$ be a function continuous on $[0,1]$ and differentiable on $(0,1)$ where $f(0)=f(1)=0$. Prove that there exists $c\in(0,1)$ such that
\[ f(x)+f^\prime(x)=0. \]
\end{exercise}

Among others:
3. Applications of MVT
3a) Darboux's Theorem
3b) L'Hopital's Rule
3c) Taylor Series
:
3a) Darboux's Theorem
I don't like the fact that the book uses the title 'continuity of derivatives'; as we've mentioned, f(x)=x² sin1/x has a discontinuous derivative
:
What the book is trying to express here is that Darboux's Theorem implies some sort of a 'intermediate value' property of derivatives that is similar to continuous functions

:
We'll hopefully finish the proofs of the three statements above for today
:
3a) Darboux's Theorem
:
This is Theorem 5.12 in the book
:
Now first and foremost, the requirement for this statement is that f must be differentiable on [a,b], not just in (a,b)
Otherwise f'(a) and f'(b) may not make sense
:
One common theme in many of these problems is to construct auxiliary functions
Suppose that f'(a)<λ<f'(b), then we construct the auxiliary function g(x)=f(x)-λx
:
Then we only need to find a point x∈(a,b) such that g'(x)=0
:
This means that we only need to find a local maximum/minimum, which by Fermat's Theorem has to be a stationary point as well
:
Now we look at the values of g near a and b
:
Exercise 1: Using the fact that g'(a)<0 and g'(b)>0, show that a and b are local maxima of g
:
Here we regard g as simply a function on [a,b], so we only need to show that a,b are maximum and corresponding semi-open neighbourhoods [a,a+ε) and (b-ε,b]
:
Let m=g'(a)<0 be the slope of the tangent at a
:
Then lim(h→0+)[g(a+h)-g(a)]/h=m<0
:
This means that there should exist δ>0 such that for 0<h<δ, [g(a+h)-g(a)]/h<m/2<0
:
Now we can rewrite the above as
g(a+h)<g(a)+mh/2
:
Since m<0 and h>0, we obtain
g(a+h)<g(a) for 0<h<δ
:
Thus this proves that x=a is a local maximum of g
A similar proof applies for x=b
:
Now since g is differentiable on [a,b], in particular it has to be continuous on [a,b]
:
Since [a,b] is compact, g([a,b]) is compact in R and thus g has both maximum and minimum values in [a,b]
:
Here we'll just focus on the minimum value
:
As we've shown, x=a is a 'strict' local maxima, in the sense that for any point x∈(a,a+ε), we actually have the strict inequality g(x)<g(a)
:
This means that x=a cannot be a local minimum
:
Similarly, x=b cannot be a local minimum, and therefore g achieves its minimum strictly inside (a,b)
:
Only then we can say that this local minimum is stationary
(This will not work otherwise; note that a and b are both local maxima but are not stationary points of g)
:
An interesting implication of Darboux's Theorem is that if f is differentiable on [a,b], then f' cannot have simple discontinuities (removable or jump discontinuities), simply because these discontinuities do not allow this 'intermediate value' property
:
However, we should recall certain pathological examples like f(x)=x² sin 1/x (f(0)=0)
Here f'(0)=lim(h→0)[x² sin 1/x-0]/x=0, but f'(x)=2x sin 1/x - cos 1/x, so f' is discontinuous at x=0
:
3b) L'Hopital's Rule (There's no 's' at the third letter)
:
First, a counterexamples
:
Let's say that we apply this rule to lim(x→∞) sin x/x
:
Then we have
lim(x→∞) sin x/x=lim(x→∞) cos x/1
:
The limit on the RHS doesn't exist because cos x oscillates between -1 and 1
:
However, the limit on the LHS does in fact exist and is equal to 0
:
So this tells us that there are certain cases where we can apply L'Hopital, and other cases where we can't
That being said, the case that we can apply the rule is actually the more useful case, so this situation does not jeopardize the effectiveness of L'Hopital
:
The entire statement is consequently rather long, so we'll split it into a few sections
:
1. f and g are differentiable in (a,b) and g'(x)≠0 in (a,b) (or at least in a small neighbourhood of a)
:
2. f(x)/g(x) is an indeterminate of the form 0/0 or ∞/∞ (Now for the second one here we only really need g(x)→∞, but if f(x) does not approach infinity then the limit would simply be zero, so L'Hopital's Rule would not be required here)
:
3. lim(x→a)f'(x)/g'(x) = A
This is the most important one
:
From this we obtain lim(x→a)f(x)/g(x) = A
:
So for example, let's say that we want to calculate the following limit:
lim(x→0) (sin x - x)/x³
:
Repeated application of L'Hopital gives
lim(x→0) (sin x - x)/x³
=lim(x→0) (cos x - 1)/3x²
=lim(x→0) -sin x/6x =-1/6
:
Now what we're really doing here is that, first we know that lim(x→0) sin x/x =1, so lim(x→0) -sin x/6x =-1/6
:
Then by L'Hopital, lim(x→0) (cos x - 1)/3x²=-1/6
:
Finally, again by L'Hopital, lim(x→0) (sin x - x)/x³=-1/6
:
So, one very important thing to take note is that if you're calculating some complicated limit and you end up with the conclusion that it doesn't exist, you must make sure that you have not used L'Hopital during the process, because the rule never applies in such situations
:
As a side note, from the above calculation we see that as x→0, sin x ≈ x-x³/6
This will later lead to the discussion of the Taylor series of sin x

Now the entire proof is quite tedious because there's actually eight main cases to think of
1. 0/0 or ∞/∞
2. a is normal or a=-∞
3. A is normal or A=±∞

We'll only prove the most basic one here:
0/0, a and A are normal
This is the case which will be required for Taylor series

First we define f(a)=g(a)=0, so that $f$ and $g$ are continuous at $x=a$

Now let $x\in(a,b)$, then $f$ and $g$ are continuous on $[a,x]$ and differentiable in $(a,x)$
:
Thus by Cauchy's Mean Value Theorem, there exists \xi∈(a,x) such that
f'(\xi)/g'(\xi)=[f(x)-f(a)]/[g(x)-g(a)]=f(x)/g(x)

For each $x$, we pick $\xi$ which satisfies the above, so that $\xi$ may be seen as a function of $x$ satisfying $a<\xi(x)<x$

Then by squeezing we have $\lim_{x\to a^+}\xi(x)=a$.

Since $\frac{f^\prime}{g^\prime}$ is continuous near $a$, the theorem regarding the limit of composite functions give
\begin{align*}
lim(x→a+) f(x)/g(x)
= lim(x→a+)f'(\xi)/g'(\xi)
= lim(x→a+)(f'/g')(\xi(x)) = A
\end{align*}

Now the same reasoning can be used for $b$ where we will use lim(x→b-) to replace all the $\lim_{x\to a^+}$, and $\xi$ will be a function which maps to $(x,b)$.

The ones with infinity are generally more complicated, but as we've mentioned we won't go through all the trouble
:
In particular, because you have the statement which works for both the left and right hand limits, L'Hopital works for ordinary limits as well
:
3c) Taylor series
:
The main expression is as follows:
f(x)=f(a)+f'(a)/1! (x-a)+f''(a)/2! (x-a)²+f'''(a)/3! (x-a)³+...
:
So for example we have the following (we've used the ones for e^x and ln x for generating functions):
e^x=1+x+x²/2!+x³/3!+...
sin x=x-x³/3!+x⁵/5!-x⁷/7!+...
cos x=1-x²/2!+x⁴/4!-x⁶/6!+...
ln(1+x)=x-x²/2+x³/3-x⁴/4+...
:
There's a lot of things to say about these equations, for example the one for ln(1+x) only works for |x|<1
:
Also, if you want the RHS of the expression to be an infinite power series, f(x) has to be smooth (infinitely differentiable)
:
Even then, the power series may never converge to f(x) at any interval, no matter how small
The most common example given here is f(x)=e^(-1/x²) (f(0)=0); the Taylor series for f(x) is just 0
:
Sorry I was talking to my mom
:
We'll extend the class until I finish this chapter
:
Now sometimes we don't actually that nice of a property for f, we're often given that fact that f is only finitely differentiable
:
Then we will have something along the lines of
f(x)≈f(a)++f'(a)/1! (x-a)+f''(a)/2! (x-a)²+...+f^(n)(a)/n! (x-a)^n (f^(n) means n-th differential)
:
There are two main forms of the statement regarding the error between the original function and the Taylor series estimate
:
The simpler form is what's known as the Peano form: Given that f is n times differentiable at a, then
f(x)=f(a)+f'(a)/1! (x-a)+f''(a)/2! (x-a)²+...+f^(n)(a)/n! (x-a)^n+o((x-a)^n)
:
To show this, we only need to show that we have the following limit:
lim(x→a) [f(x)-{f(a)+f'(a)/1! (x-a)+f''(a)/2! (x-a)²+...+f^(n)(a)/n! (x-a)^n}]/(x-a)^n=0
:
The basic idea is to use the L'Hopital Rule n times
:
The numerator becomes f^(n)(x)-f^(n)(a) which approaches 0, whereas the denominater is just n!, so the limit exists and is equal to 0
:
However, we need to verify all the necessary conditions for L'Hopital
:
Here the main problem is that we don't know if we have the 0/0 indeterminate at each step, so we'll need to check this for the k-th step where k=1,...,n
:
Fortunately, the k-th derivative of the numerator is
f^(k)(x)-f^(k)(a)-(x-a)F_k(x) where F_k is just a bunch of random stuff, so the numerator approaches 0 as x→a
The k-th derivative of the denominator is n(n-1)...(n-k+1)(x-a)^(n-k) so it also approaches 0, and we're done
:
The other form is actually a family of similar statements which gives more precise values for the error
The Peano form has a fundamental obstacle when used in approximation, we don't have any control on the size of the final term other than its asymptotic behaviour
:
We'll be talking about the one given in the book, known as the Lagrange form:
:
Given that f is n times differentiable on (a,b) such that f^(n-1) is continuous on [a,b], then
f(x)=f(a)+f'(a)/1! (x-a)+f''(a)/2! (x-a)²+...+f^(n-1)(a)/(n-1)! (x-a)^(n-1)+f^(n)(\xi)/n! (x-a)^n
:
Just like in L'Hopital, we intuitively think of (a,b) as just a very small interval at the right hand side of x=a
:
Here we are giving up on the second final term of Peano by combining it with the infinitesimal (small o) term to give an accurate description of the error
:
For the proof of this one we'll be using Cauchy's MVT
:
Fix any x in (a,b), then we construct the functions
F(t)=f(x)-[f(t)+f'(t)/1! (x-t)+f''(t)/2! (x-t)²+...+f^(n-1)(t)/(n-1)! (x-t)^(n-1)]
G(t)=(x-t)^n
:
We calculate F'(t) as follows:
-[f'(t)
+f''(t)/1!-f'(t)
+f'''(t)/2!-f''(t)/1!
+...
+f^(n)(t)/(n-1)! (x-t)^(n-1)-f^(n-1)(t)/(n-2)! (x-t)^(n-2)]
=-f^(n)(t)/(n-1)! (x-t)^(n-1)
:
G'(t)=-n(x-t)^(n-1), so we have
F'(t)/G'(t)=f^(n)(t)/n!

:
G'(t)=-n(x-t)^(n-1), so we have
F'(t)/G'(t)=f^(n)(t)/n!
:
The main reason for why we come up with the strange-looking F and G is that we specifically swap out a for t so that F(x)=G(x)=0, in hopes of getting rid of x:
We apply Cauchy's MVT to F and G on [a,x], so that we obtain \xi∈(a,x) satisfying
F'(\xi)/G'(\xi)=[F(x)-F(a)]/[G(x)-G(a)]=F(a)/G(a)
Thus the Lagrange form of the remainder is given by F(a)=f^(n)(\xi)/n! G(a)
:
Okay that's all for today
You can look at the section about vector-valued functions, and note the similarities and differences between real functions and vector functions
:
Theorem 5.19 is important, so do go through that proof as an exercise